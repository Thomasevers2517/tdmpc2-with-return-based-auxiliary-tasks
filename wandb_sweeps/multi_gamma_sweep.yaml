# Weights & Biases sweep config for multi-gamma auxiliary experiments
# Sweep dimensions:
#   1. Number & values of auxiliary gammas (list)
#   2. multi_gamma_loss_weight
#   3. Baseline case (no auxiliaries)
#
# Fixed settings (as per request):
#   task = cartpole-swingup
#   model_size = 5
#   batch_size = 8096
#   steps = 25000
#   compile = true
#   seed = (random per run; we omit fixed seed so each agent gets a different seed via Python random if not overridden)
#
# Notes:
# - Primary discount is derived heuristically; auxiliary list should NOT repeat it.
# - Baseline included via multi_gamma_enabled=false.
# - For multi_gamma_loss_weight=0 we still enable auxiliaries to isolate purely representation effect with zero gradient weight.
# - Use command line override variables; Hydra parses flattened keys.
#
# Launch example:
#   wandb sweep wandb_sweeps/multi_gamma_sweep.yaml
#   wandb agent <entity>/<project>/<sweep_id>
# Ensure train.py handles these CLI args (already supported via Hydra overrides).

program: tdmpc2/train.py
method: random
command:
  - ${env}
  - /space/thomasevers/conda-envs/tdmpc2/bin/python
  - ${program}
  - ${args_no_hyphens}
metric:
  name: eval/episode_reward
  goal: maximize
parameters:
  # Baseline: supply empty list (no auxiliaries).
  multi_gamma_gammas:
    values:
      - []
      - [0.90]
      - [0.90, 0.95]
      - [0.90, 0.95, 0.80, 0.99]
  multi_gamma_loss_weight:
    values: [0.1, 0.5]

  # Task sweep: compare two harder dynamics tasks under pixel (rgb) observations
  task:
    values: [acrobot-swingup, reacher-easy]

  # Fixed / near-fixed
  model_size:
    value: 5
  obs:
    value: rgb
  batch_size:
    value: 256
  steps:
    value: 250000
  eval_freq:
    value: 1000
  compile:
    value: true
  seed:
    distribution: int_uniform
    min: 1
    max: 1000000
  enable_wandb:
    value: true
  wandb_project:
    value: auxilary
  wandb_entity:
    value: thomasevers9
  save_video:
    value: false

# Baseline runs use multi_gamma_gammas=[] (no auxiliary discounts).
