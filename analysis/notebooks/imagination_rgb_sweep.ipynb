{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35cb8119",
   "metadata": {},
   "source": [
    "# Midterm Sweep — Imagination RGB\n",
    "\n",
    "Analyze sweep `8finalsweep/imagination_rgb` using RGB observations with hyperparameters tuned for proprioceptive control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c124aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "REPO_ROOT = NOTEBOOK_DIR.parent.parent\n",
    "if not (REPO_ROOT / \"analysis\" / \"tools\").exists():\n",
    "    raise RuntimeError(\"Unable to locate analysis/tools package from notebook directory\")\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a05bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from IPython.display import Image\n",
    "from analysis.tools import aggregations, baselines, naming, plotting, selection, wandb_io, paths\n",
    "from analysis.tools.paths import ensure_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4406000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_STEM = \"imagination_rgb_sweep\"\n",
    "SWEEP_ROOT = Path(\"../../sweep_list/midterm_sweep/8finalsweep/imagination_rgb\").resolve()\n",
    "SWEEP_ID = SWEEP_ROOT.joinpath(\"id.txt\").read_text().strip()\n",
    "WANDB_PROJECT = SWEEP_ROOT.joinpath(\"project.txt\").read_text().strip()\n",
    "WANDB_ENTITY = \"thomasevers9\"\n",
    "HISTORY_KEYS = [\n",
    "    \"eval/episode_reward\",\n",
    "eval/step\",\n",
    "train/step\",\n",
    "global_step\",\n",
    "total_env_steps\",\n",
    "step\",\n",
    "_step\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1fdc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_payload, manifest, data_source = wandb_io.fetch_sweep_runs(\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    sweep_id=SWEEP_ID,\n",
    "    history_keys=HISTORY_KEYS,\n",
    "    use_cache=True,\n",
    "    force_refresh=False,\n",
    ")\n",
    "print(f\"Loaded {manifest['run_count']} runs from {data_source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_TO_COLUMNS = {\n",
    "    \"task\": \"task\",\n",
    "    \"seed\": \"seed\",\n",
    "    \"utd_ratio\": \"utd_ratio\",\n",
    "    \"model_size\": \"model_size\",\n",
    "    \"multi_gamma_gammas\": \"multi_gamma_gammas\",\n",
    "    \"num_rollouts\": \"num_rollouts\",\n",
    "    \"entropy_coef\": \"entropy_coef\",\n",
    "    \"enc_lr_scale\": \"enc_lr_scale\",\n",
    "    \"obs\": \"obs\",\n",
    "}\n",
    "\n",
    "runs_df = aggregations.runs_history_to_frame(\n",
    "    runs_payload,\n",
    "    metric_key=METRIC_KEY,\n",
    "    step_keys=STEP_KEYS,\n",
    "    config_to_columns=CONFIG_TO_COLUMNS,\n",
    ")\n",
    "runs_df[\"task_baseline\"] = runs_df[\"task\"].map(naming.wandb_task_to_baseline)\n",
    "runs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20cbee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_modes = sorted(runs_df[\"obs\"].unique())\n",
    "if not obs_modes:\n",
    "    raise ValueError(\"No observation modalities detected in runs_df\")\n",
    "if len(obs_modes) > 1:\n",
    "    raise ValueError(\n",
    "        f\"Multiple observation modalities found {obs_modes}; per-task baseline handling not yet implemented\"\n",
    "    )\n",
    "obs_mode = obs_modes[0]\n",
    "if obs_mode == \"rgb\":\n",
    "    baseline_root = baselines.PIXEL_BASELINE_ROOT\n",
    "elif obs_mode == \"state\":\n",
    "    baseline_root = baselines.STATE_BASELINE_ROOT\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported observation modality '{obs_mode}' for baselines\")\n",
    "\n",
    "baseline_tasks = sorted(runs_df[\"task_baseline\"].unique())\n",
    "available_tasks = [\n",
    "    task for task in baseline_tasks if baselines.has_task(task, root=baseline_root)\n",
    "]\n",
    "missing_tasks = sorted(set(baseline_tasks) - set(available_tasks))\n",
    "if missing_tasks:\n",
    "    print(\n",
    "        \"Skipping tasks without baseline CSV:\",\n",
    "        \", \",.join(missing_tasks),\n",
    "    )\n",
    "if not available_tasks:\n",
    "    raise ValueError(\n",
    "        f\"No baselines available under {baseline_root} for tasks {baseline_tasks}\"\n",
    "    )\n",
    "\n",
    "baseline_df = baselines.load_many(available_tasks, root=baseline_root)\n",
    "baseline_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d275a46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_dir = ensure_dir(RESULTS_DIR / \"utd_ratio\")\n",
    "variant_fig = plotting.sample_efficiency_figure(\n",
    "    runs_df,\n",
    "    metric_key=METRIC_KEY,\n",
    "    variant_column=\"utd_ratio\",\n",
    "    task_name=\"All Tasks\",\n",
    "    baseline_frame=pd.DataFrame(),\n",
    "    baseline_label=BASELINE_LABEL,\n",
    ")\n",
    "plotting.write_png(\n",
    "    variant_fig,\n",
    "    output_path=variant_dir / \"utd_ratio.png\",\n",
    ")\n",
    "utd_summary = aggregations.aggregate_at_step(\n",
    "    runs_df,\n",
    "    step_value=STEP_TARGET,\n",
    "    metric_key=METRIC_KEY,\n",
    "    group_cols=[\"utd_ratio\"],\n",
    ")\n",
    "utd_summary_path = variant_dir / f\"utd_ratio_{STEP_TARGET}.csv\"\n",
    "utd_summary.to_csv(utd_summary_path, index=False)\n",
    "utd_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d935342",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_task_set = set(baseline_df[\"task\"].unique())\n",
    "summary_tables = []\n",
    "evaluated_tasks = []\n",
    "\n",
    "for task in sorted(runs_df[\"task\"].unique()):\n",
    "    task_df = runs_df[runs_df[\"task\"] == task]\n",
    "    if task_df.empty:\n",
    "        continue\n",
    "\n",
    "    baseline_name = naming.wandb_task_to_baseline(task)\n",
    "    if baseline_name not in baseline_task_set:\n",
    "        print(f\"Skipping task {task} — baseline CSV not available\")\n",
    "        continue\n",
    "\n",
    "    baseline_task_df = baseline_df[baseline_df[\"task\"] == baseline_name]\n",
    "    if baseline_task_df.empty:\n",
    "        raise ValueError(f\"Baseline DataFrame unexpectedly missing task {baseline_name}\")\n",
    "\n",
    "    task_dir = ensure_dir(RESULTS_DIR / task)\n",
    "    encoded_fig = plotting.sample_efficiency_encoded_figure(\n",
    "        frame=task_df,\n",
    "        metric_key=METRIC_KEY,\n",
    "        task_name=task,\n",
    "        baseline_frame=baseline_task_df,\n",
    "        baseline_label=BASELINE_LABEL,\n",
    "        encodings=ENCODING_SPECS,\n",
    "    )\n",
    "    plotting.write_png(\n",
    "        encoded_fig,\n",
    "        output_path=task_dir / \"sample_efficiency.png\",\n",
    "    )\n",
    "\n",
    "    agg = aggregations.aggregate_at_step(\n",
    "        task_df,\n",
    "        step_value=STEP_TARGET,\n",
    "        metric_key=METRIC_KEY,\n",
    "        group_cols=HYPERPARAM_COLUMNS,\n",
    "    )\n",
    "    agg[\"task\"] = task\n",
    "    summary_tables.append(agg)\n",
    "    evaluated_tasks.append(task)\n",
    "\n",
    "if not summary_tables:\n",
    "    raise ValueError(\"No summary tables were generated\")\n",
    "\n",
    "summary_df = pd.concat(summary_tables, ignore_index=True)\n",
    "summary_csv_path = RESULTS_DIR / f\"reward_summary_{STEP_TARGET}.csv\"\n",
    "summary_df.to_csv(summary_csv_path, index=False)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d120a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_tasks = sorted(set(evaluated_tasks))\n",
    "if not evaluated_tasks:\n",
    "    raise ValueError(\"No tasks with baseline coverage are available for comparison\")\n",
    "\n",
    "evaluated_runs_df = runs_df[runs_df[\"task\"].isin(evaluated_tasks)]\n",
    "if evaluated_runs_df.empty:\n",
    "    raise ValueError(\"No runs found after filtering to tasks with baseline coverage\")\n",
    "\n",
    "best_result = selection.best_configuration_at_step(\n",
    "    evaluated_runs_df,\n",
    "    metric_key=METRIC_KEY,\n",
    "    step_value=STEP_TARGET,\n",
    "    hyperparam_columns=HYPERPARAM_COLUMNS,\n",
    ")\n",
    "\n",
    "best_result.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24794402",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_summary = best_result.config_summary.sort_values(\"mean_reward\", ascending=False)\n",
    "best_config_row = config_summary.iloc[0]\n",
    "best_config_mean_reward = float(best_config_row[\"mean_reward\"])\n",
    "best_config_description = \n",
    ".join(f\"{name}={best_result.config[name]}\" for name in HYPERPARAM_COLUMNS)\n",
    "config_summary.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a18450",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_names = [naming.wandb_task_to_baseline(task) for task in evaluated_tasks]\n",
    "comparison_baseline_df = baselines.load_many(baseline_names, root=baseline_root)\n",
    "comparison_baselines = {BASELINE_LABEL: comparison_baseline_df}\n",
    "\n",
    "comparison_df = selection.comparison_table(\n",
    "    model_task_summary=best_result.task_summary,\n",
    "    baselines=comparison_baselines,\n",
    "    step_value=STEP_TARGET,\n",
    "    model_label=MODEL_LABEL,\n",
    ")\n",
    "\n",
    "comparison_csv = RESULTS_DIR / f\"comparison_table_{STEP_TARGET}.csv\"\n",
    "comparison_df.to_csv(comparison_csv, index=False)\n",
    "\n",
    "footer_text = (\n",
    "    f\"Best configuration: {best_config_description} — mean reward \"\n",
    "    f\"{best_config_mean_reward:.2f} averaged across {best_result.task_summary.shape[0]} tasks.\"\n",
    " )\n",
    "\n",
    "comparison_title = f\"Imagination RGB Sweep vs TD-MPC2-Pixels @ {STEP_TARGET:,} Steps\"\n",
    "comparison_fig = plotting.comparison_table_figure(\n",
    "    comparison_df,\n",
    "    title=comparison_title,\n",
    "    footer_text=footer_text,\n",
    ")\n",
    "comparison_png = RESULTS_DIR / f\"comparison_table_{STEP_TARGET}.png\"\n",
    "plotting.write_png(comparison_fig, output_path=comparison_png)\n",
    "Image(filename=str(comparison_png))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
