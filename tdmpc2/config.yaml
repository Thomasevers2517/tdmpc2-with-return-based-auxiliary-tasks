defaults:
    # - override hydra/launcher: submitit_local
    - override hydra/launcher: basic

# environment
task: reacher-easy
obs: state
episodic: false

# evaluation
checkpoint: ???
eval_episodes: 5
eval_freq: 10000


critic_type: V  # 'V' (state value) only. 'Q' is not implemented.
pi_rollout_horizon: 1  # Number of steps to rollout for model-based policy optimization

final_rho: 0.125 # if set, overrides rho schedule such at the end of the horizon the discount is final_rho

# DEPRECATED: sac_style_td is no longer used. The SAC-style entropy subtraction
# from TD targets has been removed in favor of multi-head dynamics pessimism.
# This config key is kept for backward compatibility but has no effect.
sac_style_td: false

n_mc_samples_target: 1

###############################################
# Planner (Refactor: latent disagreement, CEM/MPPI)
# Grouped here to keep related knobs together.
# - Uses world_model.rollout_latents()
# - Training selection: value + lambda*disagreement
# - Evaluation selection: value-only (argmax)
###############################################


# Number of reward ensemble heads for pessimistic reward estimation.
# NOTE: Only num_reward_heads=1 is supported. Multiple reward heads are deprecated.
num_reward_heads: 1

# TD bootstrap mode: how value ensemble heads (Ve) and dynamics heads (H) are reduced for TD targets.
# Options:
#   'min'   - min over Ve and H (pessimistic, default, preserves original behavior)
#   'mean'  - mean over Ve and H
#   'local' - each Ve head bootstraps itself (no Ve reduction), H reduced by local_td_target_dynamics_reduction
td_bootstrap_mode: local

# Dynamics head reduction when td_bootstrap_mode='local'.
# Options:
#   'min'    - min over all dynamics heads (pessimistic)
#   'mean'   - mean over all dynamics heads
#   'single' - use a single randomly-selected dynamics head per update (cheapest)
# Only used when td_bootstrap_mode='local'.
local_td_target_dynamics_reduction: mean

# Aggregation over reward and dynamics heads when computing trajectory values for planning.
# Options: 'mean' or 'max'. Applied to both reward heads (R) and dynamics heads (H).
planner_head_reduce: max

# Evaluation-specific planner head configuration.
# If true, eval uses all dynamics heads and reduces via planner_head_reduce_eval.
# If false, eval uses only head 0 (single head, no reduction needed).
planner_use_all_heads_eval: true
planner_head_reduce_eval: min  # Reduction over heads during eval: 'min' | 'max' | 'mean'

# Aggregation over reward and dynamics heads when computing policy loss.
# Options: 'mean', 'min', or 'max'. This does NOT create multiple policy heads—
# it controls how reward/dynamics head predictions are reduced before policy optimization.
policy_head_reduce: min

# Std of Gaussian noise added to policy actions when seeding the planner.
# Applied only when use_policy=True in world_model.rollout_latents.
policy_seed_noise_std: 0.05

# Fixed std of Gaussian noise added to the planner's chosen action during training.
# This is additive exploration noise (like epsilon-greedy but Gaussian).
# Set to 0.0 to disable. Used for ablation studies comparing optimistic planning vs noise-based exploration.
planner_action_noise_std: 0.0

# Weight for latent disagreement term (training only). Set 0.0 to disable.
planner_lambda_disagreement: 0

# Weight for value disagreement term (training only). Set 0.0 to disable.
# Value disagreement is std of V estimates across all (dynamics heads × V heads).
# Most meaningful when planner_num_dynamics_heads > 1 and/or num_q > 1.
planner_lambda_value_disagreement: 0

# When true, policy-seeded trajectories are only eligible for elites during
# the first MPPI iteration; subsequent iterations rely solely on sampled rollouts.
planner_policy_elites_first_iter_only: false

# Number of dynamics heads for multi-head dynamics ensemble.
# All heads are used during training for TD targets and imagination.
# During eval, uses all heads or single head based on planner_use_all_heads_eval.
planner_num_dynamics_heads: 4

# Frozen random prior network for dynamics heads to encourage ensemble diversity.
# The prior is added BEFORE SimNorm so the main network can learn to compensate.
dynamics_prior_enabled: false      # If true, add frozen prior to each dynamics head
dynamics_prior_hidden_dim: 32      # Hidden dimension for prior MLP (small for efficiency)
dynamics_prior_scale: 1.0          # Scale factor for prior output (prior uses Tanh, so in [-1, 1])

train_act_std_coeff: 0 # multiplier of the noise added to training env actions

best_eval: true

buffer_update_interval: 1  # number of env steps between buffer additions when not at episode end

# per_alpha: 0 # 0 means uniform sampling, >0 means prioritized experience replay
# per_priority: recency # recency | consistency

utd_ratio: 4  # World model updates per environment step
value_update_freq: 2  # Value updates per env step. -1 means same as utd_ratio.
pi_update_freq: 1  # Policy updates per env step.

num_rollouts: 4 # number of parallel rollouts to do in imagination for each starting state
pred_from: rollout # rollout | true_state (controls reward/termination prediction source)

# actor-critic source settings (ONLY IMAGINE SUPPORTED CURRENTLY)
ac_source: imagine # replay_rollout | imagine (controls critic/policy source)
aux_value_source: imagine # replay_true | replay_rollout | imagine

imagination_horizon: 1 
detach_imagine_value: false # if true, detach imagined latents when using them for actor/critic updates
imagine_initial_source: replay_rollout  # replay_true | replay_rollout - whether imagination starts from true encoded latents or dynamics rollout latents (head 0)
actor_source: replay_rollout # ac | replay_rollout | replay_true (controls actor source) ac copies ac source

ema_value_planning: false # if true, use ema critic for mpc planning

steps: 100_000
batch_size: 256
reward_coef: 0.1
value_coef: 0.1
termination_coef: 1
consistency_coef: 20
policy_coef: 1

encoder_consistency_coef: 1  # small loss to backprop consistency through the encoder, dreamer style. main loss is 20

imagine_value_loss_coef_mult: 1 # weight for value loss on imagined rollouts, only used if ac_source or aux_value_source is imagine

rho: 0.5
lr: 3e-4
enc_lr_scale: 0.3
# Encoder LR step-change schedule
# At (1 - enc_lr_step_ratio) * steps, encoder LR is multiplied by enc_lr_step_scale
# Set enc_lr_step_scale=0.0 to freeze encoder (similar to old detaching behavior)
# Set enc_lr_step_scale=0.1 for 10× reduction instead of full freeze
enc_lr_step_ratio: 0.5          # Fraction of training after which to step-change LR (0.5 = halfway)
enc_lr_step_scale: 0.0          # Multiplier for encoder LR after step (0.0 = freeze)
pi_lr_scale: 1  # Policy LR = lr * pi_lr_scale. Lower than critic to stabilize training.
ensemble_lr_scaling: true  # if true, scale head LRs by ensemble size to compensate for mean-reduced gradients
grad_clip_norm: 20
tau: 0.003
policy_ema_enabled: false
policy_ema_tau: 0.05
encoder_ema_enabled: false #adds significant memory and compute overhead for encoding intensive observations like RGB
encoder_ema_tau: 0.05
discount_denom: 5
discount_min: 0.95
discount_max: 0.995
buffer_size: 1000_000

# Hot buffer (recent transitions) for mixed sampling
hot_buffer_enabled: false         # When true, also store recent transitions in a small buffer
hot_buffer_ratio: 0.01           # Fraction of each training batch drawn from the hot buffer (e.g., 0.25 for 25%)
hot_buffer_size: 5               # Capacity in transitions for the hot buffer (e.g., 500). Must be >0 when enabled

pin_memory: true
prefetch: 1

exp_name: default
data_dir: ???


# planning
eval_mpc: true
train_mpc: true
iterations: 6
num_samples: 1024
num_elites: 64
num_pi_trajs: 512
horizon: 3
min_std: 0
max_std: 1
temperature: 0.1

# actor
log_std_min: -10
log_std_max: 2
end_entropy_coeff: 1e-6
start_entropy_coeff: 3e-5
# Entropy scaling by action dimension.
# scaled_entropy = entropy * (action_dim ** entropy_action_dim_power)
# power=0: no extra scaling (entropy ~ D from Gaussian sum)
# power=1: multiply by D (entropy ~ D², original TD-MPC2 intent)
# power=0.5: compromise (entropy ~ D^1.5)
entropy_action_dim_power: 1.0
end_dynamic_entropy_ratio: 1
start_dynamic_entropy_ratio: 0.5
dynamic_entropy_schedule : exponential  # linear | exponential

hinge_power: 4
hinge_tau: 2.5
hinge_coef: 0

# dual policy (optimistic + pessimistic)
dual_policy_enabled: true        # If true, create pessimistic + optimistic policies
optimistic_entropy_mult: 1.0      # Multiply entropy coeff by this for optimistic policy
optimistic_head_reduce: max       # Reduction for optimistic policy: 'max' or 'mean'

# Value disagreement penalty for policy optimization
# Penalizes (positive λ) or rewards (negative λ) actions where dynamics heads disagree on V(z')
# Scaled by self.scale.value for unit consistency with Q estimates
policy_lambda_value_disagreement: 1           # Penalty for pessimistic policy (positive = avoid uncertainty)
optimistic_policy_lambda_value_disagreement: 0.0  # For optimistic policy (negative = seek uncertainty)

# critic
num_bins: 101
vmin: -10
vmax: +10

# architecture
model_size: ???
num_enc_layers: 3
enc_dim: 256
num_channels: 32
mlp_dim: 512
latent_dim: 512
task_dim: 96
num_q: 2
dropout: 0.01
simnorm_dim: 8

reward_dim_div: 1 # reward MLP dimension is mlp_dim divided by this value
value_dim_div: 1 # value (V) MLP dimension is mlp_dim divided by this value
# logging
wandb_project: test_auxiliary
wandb_entity: thomasevers9
wandb_silent: false
enable_wandb: true
save_csv: true
log_gradients_per_loss: true
log_detail_freq: 5000
log_freq: 200

# misc
compile: false
compile_type: reduce-overhead  # 'default', 'reduce-overhead', 'max-autotune'
save_video: false
save_agent: false
seed: 1

# debug / instrumentation
debug: false  # If true, emit a log line after every agent.update (heavy). Fail-fast constraints also validated.


nvtx_profiler: false           # If true: add lightweight NVTX record_function ranges (disables compile for clarity).

# ---------------------------------------------------------------------------
# Multi-gamma auxiliary value supervision (RESEARCH FEATURE)
# ---------------------------------------------------------------------------
# Goal: improve representation learning by supervising additional discounted
#       state-value predictions at multiple discount factors (gammas) WITHOUT
#       affecting action selection, planning, or policy targets.
# Invariants:
#   * Primary (existing) Q ensemble remains unchanged and drives planning.
#   * Auxiliary heads are training-only. Gradients flow into shared trunk.
#   * When disabled or only 1 gamma is given, behavior matches baseline.
# ---------------------------------------------------------------------------
multi_gamma_gammas: [0.9, 0.95]              # Auxiliary discounts ONLY (do not include primary). Empty => feature disabled.
joint_aux_dim_mult: 1                                     # Non-empty length ≤ 6. Primary discount auto-prepended internally.
multi_gamma_head: separate          # Head architecture: 'separate' (one Linear per aux γ). Note: 'joint' is deprecated.
multi_gamma_loss_weight: 0.01        # Scalar weight applied to MEAN auxiliary loss (after averaging over aux gammas & time).
multi_gamma_debug_logging: true    # If True: emit richer per-gamma diagnostics (heavier overhead).
multi_gamma_log_num_examples: 4    # Max examples included in debug snapshots (targets vs preds) when debug logging.
auxiliary_value_ema: true

dtype : float32

distracted_dynamic: true    # 'dynamic' (changing distractors) or 'static' (fixed distractors)
distracted_difficulty: medium   # 'easy', 'medium', 'hard'
davis_dataset_path: /users/thomasevers/users/thomas/auxilarysignalsworldmodels/tdmpc2/tdmpc2/envs/custom_envs/distracting_control/DAVIS/JPEGImages/480p  # Path to DAVIS dataset for Distracting Control tasks
# convenience
work_dir: ???
task_title: ???
multitask: ???
tasks: ???
obs_shape: ???
action_dim: ???
episode_length: ???
obs_shapes: ???
action_dims: ???
episode_lengths: ???
seed_steps: ???
bin_size: ???

# ----------------------------------------------------------------------------
# Hydra logging and run directory configuration
# Routes Python logging to both console and a train.log file in the Hydra run dir
# ----------------------------------------------------------------------------
hydra:
    job:
        chdir: true
        env_set:
            PYTHONUNBUFFERED: "1"
    run:
        dir: logs/${now:%Y%m%d_%H%M%S}
    job_logging:
        formatters:
            simple:
                format: "%(asctime)s | %(levelname)s | %(name)s | %(message)s"
                datefmt: "%H:%M:%S"
        handlers:
            file:
                class: logging.FileHandler
                formatter: simple
                filename: train.log
                mode: a
            console:
                class: logging.StreamHandler
                formatter: simple
        root:
            level: INFO
            handlers: [file, console]
