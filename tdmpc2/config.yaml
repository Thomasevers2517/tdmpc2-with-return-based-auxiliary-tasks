defaults:
    # - override hydra/launcher: submitit_local
    - override hydra/launcher: basic

# ===========================================================================
#  1. Task & Training Setup
# ===========================================================================
task: reacher-easy
obs: state
steps: 100_000
seed: 1
compile: true
compile_type: reduce-overhead  # 'default', 'reduce-overhead', 'max-autotune'
dtype: float32
debug: false  # If true, emit a log line after every agent.update (heavy).
nvtx_profiler: false  # If true: add lightweight NVTX record_function ranges (disables compile).

# ===========================================================================
#  2. Architecture
# ===========================================================================
model_size: ???
latent_dim: 512
mlp_dim: 512
enc_dim: 256
num_enc_layers: 2
num_channels: 32    # Conv channels (pixel obs only)
simnorm_dim: 8
num_bins: 101
vmin: -10
vmax: +10
dropout: 0.01
encoder_dropout: 0.0  # When > 0, encodes twice (train + eval mode for stable targets)

# --- Ensemble sizes & per-head architecture ---
planner_num_dynamics_heads: 4
# How z_rollout uses dynamics heads for value/policy training:
#   single: head-0 only (legacy, B unchanged)
#   concat: all H heads concatenated into batch (B_eff = H*B, more data)
#   split:  batch divided among heads (B_eff = B, each chunk from different head)
rollout_head_strategy: single
value_per_dynamics: false  # When true, each value head group only sees its linked dynamics head's latents
num_q: 8
num_reward_heads: 1
dynamics_num_layers: 2
dynamics_dropout: 0.0
reward_dim_div: 1
num_reward_layers: 2
value_dim_div: 1
num_value_layers: 2
reward_dropout_enabled: false

# --- Prior networks (frozen random priors for ensemble diversity) ---
prior_hidden_div: 16          # prior_hidden_dim = network_hidden_dim // div
dynamics_prior_scale: 0.1     # Prior perturbation for dynamics heads. 0 = no prior.
value_prior_scale: 0.1        # Prior perturbation for reward & value heads (distributional shift). 0 = no prior.

# ===========================================================================
#  3. Optimizer & Learning Rates
# ===========================================================================
lr: 3e-4
optimizer_type: adamw  # 'adam' or 'adamw'
weight_decay: 0  # Weight decay for AdamW (ignored for Adam)
grad_clip_norm: 20
tau: 0.003  # EMA rate for target networks
enc_lr_scale: 0.3
# Encoder LR step-change schedule:
#   At (1 - enc_lr_step_ratio) * steps, encoder LR *= enc_lr_step_scale.
enc_lr_step_ratio: 0.5   # Fraction of training after which to step-change LR
enc_lr_step_scale: 1      # Multiplier for encoder LR after step (0.0 = freeze)
pi_lr_scale: 1             # Policy LR = lr * pi_lr_scale
ensemble_lr_scaling: true  # Scale head LRs by ensemble size to compensate for mean-reduced gradients

# ===========================================================================
#  4. Training Loop (update frequencies)
# ===========================================================================
batch_size: 256
utd_ratio: 4               # World model updates per environment step
value_update_freq: 2        # Value updates per env step. -1 = same as utd_ratio.
pi_update_freq: 1           # Policy updates per env step.

# ===========================================================================
#  5. Discount & Value Estimation
# ===========================================================================
# --- Discount schedule ---
discount_denom: 5
discount_min: 0.95
discount_max: 0.995
rho: 0.5
final_rho: -1  # if >0, overrides rho schedule so end-of-horizon discount = final_rho. -1 = disabled.

# --- Critic type ---
critic_type: V  # Only 'V' is implemented.
critic_source: replay_rollout       # 'replay_rollout' (dynamics-predicted z) or 'replay_true' (encoder z) for V predictions.
critic_target_source: replay_true    # 'replay_rollout' or 'replay_true' for imagination start states (TD targets).

# --- TD targets ---
local_td_bootstrap: true  # Local: each value head bootstraps itself. Global: all heads get same target.
td_target_std_coef: 0     # TD = mu + std_coef * sigma. Also accepts "opt"/"pess" strings.
td_target_dynamics_reduction: mean  # "from_std_coef" | "min" | "max" | "mean"
td_target_use_ema_policy: false
value_std_coef_default: 1.0  # Default magnitude for "opt"/"pess" string values of std_coef.
n_mc_samples_target: 1

# --- Imagination bootstrapping ---
imagination_horizon: 1
ema_value_planning: false

# ===========================================================================
#  6. Loss Coefficients
# ===========================================================================
consistency_coef: 10
reward_coef: 0.1
value_coef: 0.1
policy_coef: 1
termination_coef: 1
encoder_consistency_coef: 0
encoder_consistency_warmup_ratio: 0.0
imagine_value_loss_coef_mult: 1

# ===========================================================================
#  7. Policy (SVG / Entropy)
# ===========================================================================
bmpc_policy_parameterization: false

log_std_min: -5
log_std_max: 1
start_entropy_coeff: 3e-5
end_entropy_coeff: 0
entropy_action_dim_power: 1.0

# Jacobian correction for tanh squashing:
#   1.0 = mathematically correct, 0.0 = legacy TD-MPC2 (no Jacobian)
jacobian_correction_scale: 1.0

# Entropy schedule
dynamic_entropy_schedule: exponential  # linear | exponential
start_dynamic_entropy_ratio: 1
end_dynamic_entropy_ratio: 1

pi_rollout_horizon: 1
pi_num_rollouts: -1            # -1 = match num_rollouts, >0 = explicit count

policy_value_std_coef: 0
policy_ema_tau: null  # null = inherit from tau

# ===========================================================================
#  8. Planner (MPPI / CEM)
# ===========================================================================
eval_mpc: true
train_mpc: true
iterations: 6
extra_iter_action_dim_threshold: 15
num_samples: 512
num_elites: 64
num_pi_trajs: 24
horizon: 3
min_std: 0
max_std: 2
temperature: 0.5
mult_by_temp: true

# --- Planner value aggregation ---
planner_value_std_coef_train: 0.3
planner_value_std_coef_eval: -1
planner_aggregate_value: false
planner_lambda_disagreement: 0
planner_policy_elites_first_iter_only: false

# ===========================================================================
#  9. Exploration & Rollouts
# ===========================================================================
num_rollouts: 4
buffer_update_interval: 1
train_act_std_coeff: 0
policy_seed_noise_std: 0.05
planner_action_noise_std: 0.0
greedy_train_action_selection: false

# ===========================================================================
# 10. Evaluation
# ===========================================================================
checkpoint: ???
eval_episodes: 5
eval_freq: 10000
eval_mean_head_reduce: false
best_eval: true

# ===========================================================================
# 11. Replay Buffer
# ===========================================================================
buffer_size: 1000_000
hot_buffer_enabled: false
hot_buffer_ratio: 0.01
hot_buffer_size: 50
pin_memory: true
prefetch: 1

# ===========================================================================
# 12. Logging & W&B
# ===========================================================================
wandb_project: tdmpc2-tdmpc2
wandb_entity: thomasevers9
wandb_silent: false
enable_wandb: true
save_csv: true
log_gradients_per_loss: true
log_detail_freq: 5000
log_freq: 200
save_video: false
save_agent: false
exp_name: default
data_dir: ???

# ===========================================================================
# 13. Dual Policy (optimistic + pessimistic â€” research feature)
# ===========================================================================
dual_policy_enabled: false
optimistic_policy_value_std_coef: 0
optimistic_entropy_mult: 100

normalize_rho_weights: true

# ===========================================================================
# 14. Environment-Specific
# ===========================================================================
episodic: false
knn_entropy_k: 5
knn_entropy_dim: 512
distracted_dynamic: true
distracted_difficulty: medium
davis_dataset_path: /users/thomasevers/users/thomas/auxilarysignalsworldmodels/tdmpc2/tdmpc2/envs/custom_envs/distracting_control/DAVIS/JPEGImages/480p

# ===========================================================================
# Computed at parse time (do not set manually)
# ===========================================================================
work_dir: ???
task_title: ???
obs_shape: ???
action_dim: ???
episode_length: ???
seed_steps: ???
bin_size: ???

# ===========================================================================
# Hydra
# ===========================================================================
hydra:
    job:
        chdir: true
        env_set:
            PYTHONUNBUFFERED: "1"
    run:
        dir: logs/${now:%Y%m%d_%H%M%S}
    job_logging:
        formatters:
            simple:
                format: "%(asctime)s | %(levelname)s | %(name)s | %(message)s"
                datefmt: "%H:%M:%S"
        handlers:
            file:
                class: logging.FileHandler
                formatter: simple
                filename: train.log
                mode: a
            console:
                class: logging.StreamHandler
                formatter: simple
        root:
            level: INFO
            handlers: [file, console]
