defaults:
    # - override hydra/launcher: submitit_local
    - override hydra/launcher: basic

# ===========================================================================
# Core Training
# ===========================================================================
task: reacher-easy
obs: state
steps: 100_000
batch_size: 256
seed: 1
compile: true
compile_type: reduce-overhead  # 'default', 'reduce-overhead', 'max-autotune'
dtype: float32
debug: false  # If true, emit a log line after every agent.update (heavy).
nvtx_profiler: false  # If true: add lightweight NVTX record_function ranges (disables compile).

# ===========================================================================
# Discount & Returns
# ===========================================================================
discount_denom: 5
discount_min: 0.95
discount_max: 0.995
rho: 0.5
final_rho: -1  # if >0, overrides rho schedule so end-of-horizon discount = final_rho. -1 = disabled.

# ===========================================================================
# Optimization
# ===========================================================================
lr: 3e-4
optimizer_type: adamw  # 'adam' or 'adamw'
weight_decay: 0  # Weight decay for AdamW (ignored for Adam)
tau: 0.003
grad_clip_norm: 20
enc_lr_scale: 0.3
# Encoder LR step-change schedule:
#   At (1 - enc_lr_step_ratio) * steps, encoder LR *= enc_lr_step_scale.
enc_lr_step_ratio: 0.5   # Fraction of training after which to step-change LR
enc_lr_step_scale: 1      # Multiplier for encoder LR after step (0.0 = freeze)
pi_lr_scale: 1             # Policy LR = lr * pi_lr_scale
ensemble_lr_scaling: true  # Scale head LRs by ensemble size to compensate for mean-reduced gradients
utd_ratio: 4               # World model updates per environment step
value_update_freq: 2        # Value updates per env step. -1 = same as utd_ratio.
pi_update_freq: 1           # Policy updates per env step.

# ===========================================================================
# Loss Coefficients
# ===========================================================================
reward_coef: 0.1
value_coef: 0.1
consistency_coef: 10
termination_coef: 1
policy_coef: 1
encoder_consistency_coef: 0
encoder_consistency_warmup_ratio: 0.0
imagine_value_loss_coef_mult: 1

# ===========================================================================
# Architecture
# ===========================================================================
model_size: ???
latent_dim: 512
mlp_dim: 512
enc_dim: 256
num_enc_layers: 2
num_channels: 32    # Conv channels (pixel obs only)
task_dim: 96
simnorm_dim: 8
num_bins: 101
vmin: -10
vmax: +10
dropout: 0.01
encoder_dropout: 0.0  # When > 0, encodes twice (train + eval mode for stable targets)

# ===========================================================================
# Ensemble (Dynamics, Reward, Value)
# ===========================================================================
planner_num_dynamics_heads: 4
num_q: 8
num_reward_heads: 1
dynamics_num_layers: 2
dynamics_dropout: 0.0
reward_dim_div: 1
num_reward_layers: 2
value_dim_div: 1
num_value_layers: 2
reward_dropout_enabled: false

# ===========================================================================
# Prior Networks (frozen random priors for ensemble diversity)
# ===========================================================================
prior_hidden_div: 16   # prior_hidden_dim = network_hidden_dim // div
prior_scale: 0.1       # Per-dimension perturbation magnitude. 0 = no prior.
prior_logit_scale: 1.0 # Multiplier for prior two-hot logits

# ===========================================================================
# Critic & TD Targets
# ===========================================================================
critic_type: V  # Only 'V' is implemented.

# What latent states V learns on vs where TD targets start:
critic_source: replay_rollout       # replay_rollout | replay_true
critic_target_source: replay_true   # replay_true | replay_rollout

# Local: each value head bootstraps itself. Global: all heads get same target.
local_td_bootstrap: true

# TD = mu + std_coef * sigma. Also accepts "opt"/"pess" strings.
td_target_std_coef: 0
td_target_dynamics_reduction: mean  # "from_std_coef" | "min" | "max" | "mean"
td_target_use_all_dynamics_heads: true
td_target_use_ema_policy: false

imagination_horizon: 1
detach_imagine_value: false
ema_value_planning: false

# Default magnitude for "opt"/"pess" string values of std_coef.
value_std_coef_default: 1.0
n_mc_samples_target: 1

# ===========================================================================
# Policy (SVG / Entropy / Parameterization)
# ===========================================================================
policy_optimization_method: svg  # 'svg' | 'distillation' | 'both'
bmpc_policy_parameterization: false

log_std_min: -5
log_std_max: 1
start_entropy_coeff: 3e-5
end_entropy_coeff: 0
entropy_action_dim_power: 1.0

# Jacobian correction for tanh squashing:
#   1.0 = mathematically correct, 0.0 = legacy TD-MPC2 (no Jacobian)
jacobian_correction_scale: 1.0

# Entropy schedule
dynamic_entropy_schedule: exponential  # linear | exponential
start_dynamic_entropy_ratio: 1
end_dynamic_entropy_ratio: 1

pi_rollout_horizon: 1
pi_multi_rollout: false

policy_value_std_coef: 0
policy_trust_region_coef: 0.0
policy_ema_tau: null  # null = inherit from tau

# ===========================================================================
# Planner (MPPI/CEM)
# ===========================================================================
eval_mpc: true
train_mpc: true
iterations: 6
extra_iter_action_dim_threshold: 15
num_samples: 512
num_elites: 64
num_pi_trajs: 24
horizon: 3
min_std: 0
max_std: 2
temperature: 0.5
mult_by_temp: true

planner_value_std_coef_train: 0.3
planner_value_std_coef_eval: -1
planner_aggregate_value: false
planner_use_all_heads_eval: true
policy_seed_noise_std: 0.05
planner_action_noise_std: 0.0
greedy_train_action_selection: false
planner_lambda_disagreement: 0
planner_policy_elites_first_iter_only: false

# ===========================================================================
# Exploration & Rollouts
# ===========================================================================
train_act_std_coeff: 0
num_rollouts: 4
pred_from: rollout  # rollout | true_state
buffer_update_interval: 1

# ===========================================================================
# Dual Policy (optimistic + pessimistic)
# ===========================================================================
dual_policy_enabled: false
optimistic_policy_value_std_coef: 0
optimistic_entropy_mult: 100
optimistic_policy_optimization_method: same

# ===========================================================================
# Evaluation
# ===========================================================================
checkpoint: ???
eval_episodes: 5
eval_freq: 10000
eval_mean_head_reduce: false
best_eval: true

# ===========================================================================
# Replay Buffer
# ===========================================================================
buffer_size: 1000_000
hot_buffer_enabled: false
hot_buffer_ratio: 0.01
hot_buffer_size: 50
pin_memory: true
prefetch: 1

# ===========================================================================
# Logging & W&B
# ===========================================================================
wandb_project: tdmpc2-tdmpc2
wandb_entity: thomasevers9
wandb_silent: false
enable_wandb: true
save_csv: true
log_gradients_per_loss: true
log_detail_freq: 5000
log_freq: 200
save_video: false
save_agent: false
exp_name: default
data_dir: ???

# ===========================================================================
# Reanalyze / Distillation (mostly unused with SVG baseline)
# ===========================================================================
reanalyze_interval: 10
reanalyze_batch_size: 60
reanalyze_horizon: null
reanalyze_iterations: null
reanalyze_num_samples: null
reanalyze_num_elites: null
reanalyze_num_pi_trajs: null
reanalyze_temperature: null
reanalyze_value_std_coef: 0.0
reanalyze_use_chosen_action: false
reanalyze_slice_mode: true
initial_expert_from_behavior: true
expert_std_scale: 1.5
normalize_rho_weights: true
min_expert_std: 0.1
policy_svg_distill_ratio: 0.5
fix_kl_order: false
kl_scale_min: 0.1

# ===========================================================================
# Multi-Gamma Auxiliary Value Supervision (Research Feature)
# ===========================================================================
multi_gamma_gammas: [0.9, 0.95]
joint_aux_dim_mult: 1
multi_gamma_head: separate
multi_gamma_loss_weight: 0       # 0 = disabled
multi_gamma_debug_logging: true
multi_gamma_log_num_examples: 4
auxiliary_value_ema: true

# ===========================================================================
# Environment-Specific
# ===========================================================================
episodic: false
knn_entropy_k: 5
knn_entropy_dim: 512
distracted_dynamic: true
distracted_difficulty: medium
davis_dataset_path: /users/thomasevers/users/thomas/auxilarysignalsworldmodels/tdmpc2/tdmpc2/envs/custom_envs/distracting_control/DAVIS/JPEGImages/480p

# ===========================================================================
# Deprecated / Legacy (no effect)
# ===========================================================================
sac_style_td: false
hinge_power: 4
hinge_tau: 2.5
hinge_coef: 0

# ===========================================================================
# Computed at parse time (do not set manually)
# ===========================================================================
work_dir: ???
task_title: ???
multitask: ???
tasks: ???
obs_shape: ???
action_dim: ???
episode_length: ???
obs_shapes: ???
action_dims: ???
episode_lengths: ???
seed_steps: ???
bin_size: ???

# ===========================================================================
# Hydra logging and run directory configuration
# ===========================================================================
hydra:
    job:
        chdir: true
        env_set:
            PYTHONUNBUFFERED: "1"
    run:
        dir: logs/${now:%Y%m%d_%H%M%S}
    job_logging:
        formatters:
            simple:
                format: "%(asctime)s | %(levelname)s | %(name)s | %(message)s"
                datefmt: "%H:%M:%S"
        handlers:
            file:
                class: logging.FileHandler
                formatter: simple
                filename: train.log
                mode: a
            console:
                class: logging.StreamHandler
                formatter: simple
        root:
            level: INFO
            handlers: [file, console]
