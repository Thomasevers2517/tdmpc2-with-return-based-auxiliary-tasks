defaults:
    # - override hydra/launcher: submitit_local
    - override hydra/launcher: basic

# environment
task: reacher-easy
obs: rgb
episodic: false

# evaluation
checkpoint: ???
eval_episodes: 10
eval_freq: 50000
best_eval: false



# training
utd_ratio: 1
ac_utd_multiplier: 1
reset_agent_freq: 400_000_000   # number of ac updates between resets of actor and critic networks so steps / (utd_ratio * ac_utd_multiplier) is number of env steps between resets

imagination_horizon: 3
imagine_num_starts: -1
imagine_value_loss_coef: 0 # 0 to disable, 0.1 - 0.01 seems reasonable
num_rollouts: 4 # number of parallel rollouts to do in imagination for each starting state
detach_imagine_value: true  # if true, detach the latents used for imagine value loss calculation. This is to prevent gradients from the imagine value loss to flow into the encoder and dynamics model. This is to isolate the effect of the imagine value loss on the value function only.
#Origianl TD-MPC did not use the true state for predictions, they used the rollouts. 
pred_from: rollout # rollout, true_state, both 

#if both is selected
split_batch: true # If Both, do we split the batch in two, one half for rollout, one half for true state
rollout_fraction: 0.5 # If Both, what how much is the rollout weighted


steps: 4_000_000
batch_size: 256
reward_coef: 0.1
value_coef: 0.1
termination_coef: 1
consistency_coef: 20
policy_coef: 1

encoder_consistency_coef: 0  # small loss to backprop consistency through the encoder, dreamer style. main loss is 20

rho: 0.5
lr: 3e-4
enc_lr_scale: 0.3
grad_clip_norm: 20
tau: 0.01
policy_ema_enabled: false
policy_ema_tau: 0.05
encoder_ema_enabled: false #adds significant memory and compute overhead for encoding intensive observations like RGB
encoder_ema_tau: 0.05
discount_denom: 5
discount_min: 0.95
discount_max: 0.995
buffer_size: 100_000

pin_memory: true
prefetch: 4

exp_name: default
data_dir: ???

# network reset policy
reset:
    fallbacks:
        shrink_alpha: 0.9            # fallback α for shrink-perturb resets
        shrink_noise_std: 0.05       # fallback σ for shrink-perturb resets
    actor_critic:
        type: none                   # none | full | shrink_perturb (policy + all Q heads)
        layers: 1                    # -1 ⇒ all learnable layers, otherwise last-k layers
        alpha: null                  # optional override when type == shrink_perturb
        noise_std: null              # optional override when type == shrink_perturb
    encoder_dynamics:
        type: none         # none | full | shrink_perturb (encoder + dynamics)
        layers: -1                   # applies to each sub-encoder independently
        alpha: null                  # optional override when type == shrink_perturb
        noise_std: null              # optional override when type == shrink_perturb

# planning
eval_mpc: true
train_mpc: true
iterations: 6
num_samples: 512
num_elites: 64
num_pi_trajs: 24
horizon: 3
min_std: 0.05
max_std: 2
temperature: 0.5

# actor
log_std_min: -10
log_std_max: 2
entropy_coef: 1e-4

# critic
num_bins: 101
vmin: -10
vmax: +10

# architecture
model_size: ???
num_enc_layers: 2
enc_dim: 256
num_channels: 32
mlp_dim: 512
latent_dim: 512
task_dim: 96
num_q: 5
dropout: 0.01
simnorm_dim: 8

# logging
wandb_project: test_auxiliary
wandb_entity: thomasevers9
wandb_silent: false
enable_wandb: true
save_csv: true
log_gradients_per_loss: true
log_detail_freq: 1000

# misc
compile: false
compile_type: reduce-overhead  # 'default', 'reduce-overhead', 'max-autotune'
save_video: false
save_agent: false
seed: 1


nvtx_profiler: false           # If true: add lightweight NVTX record_function ranges (disables compile for clarity).

# ---------------------------------------------------------------------------
# Multi-gamma auxiliary value supervision (RESEARCH FEATURE)
# ---------------------------------------------------------------------------
# Goal: improve representation learning by supervising additional discounted
#       state-value predictions at multiple discount factors (gammas) WITHOUT
#       affecting action selection, planning, or policy targets.
# Invariants:
#   * Primary (existing) Q ensemble remains unchanged and drives planning.
#   * Auxiliary heads are training-only. Gradients flow into shared trunk.
#   * When disabled or only 1 gamma is given, behavior matches baseline.
# ---------------------------------------------------------------------------
multi_gamma_gammas: [0.9, 0.95]              # Auxiliary discounts ONLY (do not include primary). Empty => feature disabled.
joint_aux_dim_mult: 1                                     # Non-empty length ≤ 6. Primary discount auto-prepended internally.
multi_gamma_head: joint             # Head architecture: 'joint' (single Linear -> G_aux*K) or 'separate' (one Linear per aux γ).
multi_gamma_loss_weight: 0.1        # Scalar weight applied to MEAN auxiliary loss (after averaging over aux gammas & time).
multi_gamma_debug_logging: true    # If True: emit richer per-gamma diagnostics (heavier overhead).
multi_gamma_log_num_examples: 4    # Max examples included in debug snapshots (targets vs preds) when debug logging.
auxiliary_value_ema: true

dtype : float32

distracted_dynamic: true    # 'dynamic' (changing distractors) or 'static' (fixed distractors)
distracted_difficulty: medium   # 'easy', 'medium', 'hard'
davis_dataset_path: /users/thomasevers/users/thomas/auxilarysignalsworldmodels/tdmpc2/tdmpc2/envs/custom_envs/distracting_control/DAVIS/JPEGImages/480p  # Path to DAVIS dataset for Distracting Control tasks
# convenience
work_dir: ???
task_title: ???
multitask: ???
tasks: ???
obs_shape: ???
action_dim: ???
episode_length: ???
obs_shapes: ???
action_dims: ???
episode_lengths: ???
seed_steps: ???
bin_size: ???

# ----------------------------------------------------------------------------
# Hydra logging and run directory configuration
# Routes Python logging to both console and a train.log file in the Hydra run dir
# ----------------------------------------------------------------------------
hydra:
    job:
        chdir: true
        env_set:
            PYTHONUNBUFFERED: "1"
    run:
        dir: logs/${now:%Y%m%d_%H%M%S}
    job_logging:
        formatters:
            simple:
                format: "%(asctime)s | %(levelname)s | %(name)s | %(message)s"
                datefmt: "%H:%M:%S"
        handlers:
            file:
                class: logging.FileHandler
                formatter: simple
                filename: train.log
                mode: a
            console:
                class: logging.StreamHandler
                formatter: simple
        root:
            level: INFO
            handlers: [file, console]
