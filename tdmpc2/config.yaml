defaults:
    # - override hydra/launcher: submitit_local
    - override hydra/launcher: basic

# environment
task: reacher-easy
obs: state
episodic: false

# evaluation
checkpoint: ???
eval_episodes: 10
eval_freq: 10000

best_eval: false

detach_encoder_ratio: 0.0  # fraction of training steps after which to detach encoder gradients

search_pi_std_mult: 1.0  # multiplier for noise added during seed action sampling in MPC planning
# training
buffer_update_interval: -1  # number of env steps between buffer additions when not at episode end

# per_alpha: 0 # 0 means uniform sampling, >0 means prioritized experience replay
# per_priority: recency # recency | consistency

utd_ratio: 1
ac_utd_multiplier: 1
reset_agent_freq: 400_000_000   # number of ac updates between resets of actor and critic networks so steps / (utd_ratio * ac_utd_multiplier) is number of env steps between resets

num_rollouts: 4 # number of parallel rollouts to do in imagination for each starting state
pred_from: rollout # rollout, true_state, both (controls reward/termination prediction source)
split_batch: true # If pred_from == both, split batch evenly between true and rollout predictions
ac_source: imagine # replay_rollout | imagine (controls critic/policy source)
aux_value_source: replay_true # replay_true | replay_rollout | imagine
imagination_horizon: 1 
detach_imagine_value: false # if true, detach imagined latents when using them for actor/critic updates
actor_source: replay_rollout # ac | replay_rollout | replay_true (controls actor source) ac copies ac source

ema_value_planning: false # if true, use ema critic for mpc planning

steps: 4_000_000
batch_size: 256
reward_coef: 0.1
value_coef: 0.1
termination_coef: 1
consistency_coef: 20
policy_coef: 1

encoder_consistency_coef: 1  # small loss to backprop consistency through the encoder, dreamer style. main loss is 20

imagine_value_loss_coef_mult: 1 # weight for value loss on imagined rollouts, only used if ac_source or aux_value_source is imagine

rho: 0.5
lr: 3e-4
enc_lr_scale: 0.3
grad_clip_norm: 20
tau: 0.01
aux_value_tau: 0.01
policy_ema_enabled: false
policy_ema_tau: 0.05
encoder_ema_enabled: false #adds significant memory and compute overhead for encoding intensive observations like RGB
encoder_ema_tau: 0.05
discount_denom: 5
discount_min: 0.95
discount_max: 0.995
buffer_size: 1000_000

pin_memory: true
prefetch: 4

exp_name: default
data_dir: ???

# network reset policy
reset:
    fallbacks:
        shrink_alpha: 0.9            # fallback α for shrink-perturb resets
        shrink_noise_std: 0.05       # fallback σ for shrink-perturb resets
    actor_critic:
        type: none                   # none | full | shrink_perturb (policy + all Q heads)
        layers: 1                    # -1 ⇒ all learnable layers, otherwise last-k layers
        alpha: null                  # optional override when type == shrink_perturb
        noise_std: null              # optional override when type == shrink_perturb
    encoder_dynamics:
        type: none         # none | full | shrink_perturb (encoder + dynamics)
        layers: -1                   # applies to each sub-encoder independently
        alpha: null                  # optional override when type == shrink_perturb
        noise_std: null              # optional override when type == shrink_perturb

# planning
eval_mpc: true
train_mpc: true
iterations: 6
num_samples: 512
num_elites: 64
num_pi_trajs: 24
horizon: 3
min_std: 0.05
max_std: 2
temperature: 0.5

# actor
log_std_min: -10
log_std_max: 2
entropy_coef: 1e-4
hinge_power: 4
hinge_tau: 2.5
hinge_coef: 0.01

# critic
num_bins: 101
vmin: -10
vmax: +10

# architecture
model_size: ???
num_enc_layers: 2
enc_dim: 256
num_channels: 32
mlp_dim: 512
latent_dim: 512
task_dim: 96
num_q: 5
dropout: 0.01
simnorm_dim: 8
reward_dim_div: 1 # reward MLP dimension is mlp_dim divided by this value
# logging
wandb_project: test_auxiliary
wandb_entity: thomasevers9
wandb_silent: false
enable_wandb: true
save_csv: true
log_gradients_per_loss: true
log_detail_freq: 1000
log_freq: 50

# misc
compile: false
compile_type: reduce-overhead  # 'default', 'reduce-overhead', 'max-autotune'
save_video: false
save_agent: false
seed: 1


nvtx_profiler: false           # If true: add lightweight NVTX record_function ranges (disables compile for clarity).

# ---------------------------------------------------------------------------
# Multi-gamma auxiliary value supervision (RESEARCH FEATURE)
# ---------------------------------------------------------------------------
# Goal: improve representation learning by supervising additional discounted
#       state-value predictions at multiple discount factors (gammas) WITHOUT
#       affecting action selection, planning, or policy targets.
# Invariants:
#   * Primary (existing) Q ensemble remains unchanged and drives planning.
#   * Auxiliary heads are training-only. Gradients flow into shared trunk.
#   * When disabled or only 1 gamma is given, behavior matches baseline.
# ---------------------------------------------------------------------------
multi_gamma_gammas: [0.9, 0.95]              # Auxiliary discounts ONLY (do not include primary). Empty => feature disabled.
joint_aux_dim_mult: 1                                     # Non-empty length ≤ 6. Primary discount auto-prepended internally.
multi_gamma_head: joint             # Head architecture: 'joint' (single Linear -> G_aux*K) or 'separate' (one Linear per aux γ).
multi_gamma_loss_weight: 0.1        # Scalar weight applied to MEAN auxiliary loss (after averaging over aux gammas & time).
multi_gamma_debug_logging: true    # If True: emit richer per-gamma diagnostics (heavier overhead).
multi_gamma_log_num_examples: 4    # Max examples included in debug snapshots (targets vs preds) when debug logging.
auxiliary_value_ema: true

dtype : float32

distracted_dynamic: true    # 'dynamic' (changing distractors) or 'static' (fixed distractors)
distracted_difficulty: medium   # 'easy', 'medium', 'hard'
davis_dataset_path: /users/thomasevers/users/thomas/auxilarysignalsworldmodels/tdmpc2/tdmpc2/envs/custom_envs/distracting_control/DAVIS/JPEGImages/480p  # Path to DAVIS dataset for Distracting Control tasks
# convenience
work_dir: ???
task_title: ???
multitask: ???
tasks: ???
obs_shape: ???
action_dim: ???
episode_length: ???
obs_shapes: ???
action_dims: ???
episode_lengths: ???
seed_steps: ???
bin_size: ???

# ----------------------------------------------------------------------------
# Hydra logging and run directory configuration
# Routes Python logging to both console and a train.log file in the Hydra run dir
# ----------------------------------------------------------------------------
hydra:
    job:
        chdir: true
        env_set:
            PYTHONUNBUFFERED: "1"
    run:
        dir: logs/${now:%Y%m%d_%H%M%S}
    job_logging:
        formatters:
            simple:
                format: "%(asctime)s | %(levelname)s | %(name)s | %(message)s"
                datefmt: "%H:%M:%S"
        handlers:
            file:
                class: logging.FileHandler
                formatter: simple
                filename: train.log
                mode: a
            console:
                class: logging.StreamHandler
                formatter: simple
        root:
            level: INFO
            handlers: [file, console]
