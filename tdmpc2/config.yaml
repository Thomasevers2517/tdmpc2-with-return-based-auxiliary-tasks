defaults:
    # - override hydra/launcher: submitit_local
    - override hydra/launcher: basic

# environment
task: reacher-easy
obs: state
episodic: false

# evaluation
checkpoint: ???
eval_episodes: 5
eval_freq: 10000
eval_mean_head_reduce: false  # If true, also run evaluation with mean head reduction (in addition to default min)

# KNN entropy for measuring observation diversity in replay buffer
knn_entropy_k: 5  # k for k-th nearest neighbor entropy estimator
knn_entropy_dim: 512  # output dimension of frozen random encoder

critic_type: V  # 'V' (state value) only. 'Q' is not implemented.
pi_rollout_horizon: 1  # Number of steps to rollout for model-based policy optimization

final_rho: 0.125 # if set, overrides rho schedule such at the end of the horizon the discount is final_rho

# DEPRECATED: sac_style_td is no longer used. The SAC-style entropy subtraction
# from TD targets has been removed in favor of multi-head dynamics pessimism.
# This config key is kept for backward compatibility but has no effect.
sac_style_td: false

n_mc_samples_target: 1

###############################################
# Planner (Refactor: latent disagreement, CEM/MPPI)
# Grouped here to keep related knobs together.
# - Uses world_model.rollout_latents()
# - Training selection: value + lambda*disagreement
# - Evaluation selection: value-only (argmax)
###############################################


# Number of reward ensemble heads for pessimistic reward estimation.
num_reward_heads: 2

# ---------------------------------------------------------------------------
# Correct Optimism: Head reduction via per-step std, then min/max/mean over dynamics
# ---------------------------------------------------------------------------
# For each dynamics head h, compute:
#   σ_h = sum_t(γ^t * σ^r_h,t) + γ^T * σ^v_h   (reward std + value std)
#   Q_h = μ_h + std_coef × σ_h
# Then reduce over dynamics heads:
#   std_coef > 0 (opt): max over H (optimistic)
#   std_coef < 0 (pess): min over H (pessimistic)  
#   std_coef = 0: mean over H (neutral)
#
# std_coef can be:
#   - numeric: use that value directly (e.g., -1.0, 0.5, 2.0)
#   - "opt": use +value_std_coef_default (optimistic)
#   - "pess": use -value_std_coef_default (pessimistic)
# ---------------------------------------------------------------------------

# Default magnitude for std_coef when using "opt"/"pess" strings.
# With "opt" -> +value_std_coef_default, "pess" -> -value_std_coef_default.
# Allows sweeping magnitude while keeping opt/pess semantics.
value_std_coef_default: 1.0

# TD target: coefficient for pessimism/optimism in TD target computation
# Controls reward uncertainty adjustment: TD = μ + std_coef * σ
# Negative: pessimistic about reward uncertainty (subtract std)
# Positive: optimistic about reward uncertainty (add std)
# Zero: neutral (mean only)
# Can also use "pess" or "opt" strings.
td_target_std_coef: 0

# How to reduce over dynamics heads H in TD target computation.
# Options:
#   - "from_std_coef": use sign of td_target_std_coef (default, legacy)
#       > 0: max over H, < 0: min over H, = 0: mean over H
#   - "min": always min over dynamics heads (pessimistic about dynamics)
#   - "max": always max over dynamics heads (optimistic about dynamics)
#   - "mean": always mean over dynamics heads (neutral about dynamics)
# Use case: td_target_std_coef=-1 (pessimistic about reward) with
# td_target_dynamics_reduction="mean" to NOT be pessimistic about dynamics.
td_target_dynamics_reduction: mean

# Local vs global TD bootstrapping:
# - false (global): reduce across ALL heads (Ve × R × H), all value heads get same target
# - true (local): reduce across (R × H) only, each value head bootstraps itself
local_td_bootstrap: true

# Whether to use all dynamics heads or single random head for TD target imagination
# When false, uses single random head (cheaper but H-std is 0)
td_target_use_all_dynamics_heads: true

# Policy value std coefficients for pessimistic/optimistic policy optimization
# Pessimistic policy: "pess" or negative value (avoid uncertainty, min over dynamics)
# Optimistic policy: "opt" or positive value (seek uncertainty, max over dynamics)
# Zero: mean only (mean over dynamics)
policy_value_std_coef: 0
optimistic_policy_value_std_coef: 1

# Planner value std coefficients (separate for train vs eval)
# Train: "opt" or positive (encourage exploration, max over dynamics)
# Eval: "pess" or negative (robust action selection, min over dynamics)
planner_value_std_coef_train: 1
planner_value_std_coef_eval: -1

# Aggregate horizon (λ-style compound return) for planner value estimation.
# If true, compute returns at each intermediate horizon τ ∈ {1, ..., T}:
#   G_τ = Σ_{t=0}^{τ-1}(α_t × r_t) + α_τ × V(z_τ)
# And take the uniform average: G = (1/T) Σ_τ G_τ
# This reduces model exploitation by averaging over different bootstrap depths.
# Cost: T× more V calls (mitigated by batching all horizons in one forward pass).
planner_aggregate_value: false

# Evaluation-specific planner head configuration.
# If true, eval uses all dynamics heads.
# If false, eval uses only head 0 (single head, no reduction needed).
planner_use_all_heads_eval: true

# Std of Gaussian noise added to policy actions when seeding the planner.
# Applied only when use_policy=True in world_model.rollout_latents.
policy_seed_noise_std: 0.05

# Fixed std of Gaussian noise added to the planner's chosen action during training.
# This is additive exploration noise (like epsilon-greedy but Gaussian).
# Set to 0.0 to disable. Used for ablation studies comparing optimistic planning vs noise-based exploration.
planner_action_noise_std: 0.0

# Greedy (argmax) vs softmax sampling for training action selection.
# If true: use argmax over elite scores (deterministic, like eval mode).
# If false: use softmax sampling from elites (stochastic exploration).
# For BMPC-style policy parameterization, greedy selection may work better
# since exploration comes from policy noise rather than planner sampling.
greedy_train_action_selection: false

# Weight for latent disagreement term (training only). Set 0.0 to disable.
planner_lambda_disagreement: 0

# When true, policy-seeded trajectories are only eligible for elites during
# the first MPPI iteration; subsequent iterations rely solely on sampled rollouts.
planner_policy_elites_first_iter_only: false

# Number of dynamics heads for multi-head dynamics ensemble.
# All heads are used during training for TD targets and imagination.
# During eval, uses all heads or single head based on planner_use_all_heads_eval.
planner_num_dynamics_heads: 2

# Frozen random prior networks for ensemble diversity (dynamics, value, reward).
# Prior is L2-normalized and added to main network output.
# prior_scale is dimension-aware: 1.0 means ~1.0 perturbation per output dimension.
# Typical logit range is ~[-3, 3], so prior_scale in {0.1, 1.0, 3.0} is reasonable.
prior_hidden_div: 16                # prior_hidden_dim = network_hidden_dim // div
prior_scale: 0.1                   # Scalar value the prior network targets (symlog scale). 0 = no prior.
prior_logit_scale: 1.0             # Multiplier for prior two-hot logits (controls influence strength on output distribution).
dynamics_dropout: 0.0              # Dropout rate for dynamics head (applied to first hidden layer)
dynamics_num_layers: 2             # Number of hidden layers in dynamics head MLP

train_act_std_coeff: 0 # multiplier of the noise added to training env actions

best_eval: true

buffer_update_interval: 1  # number of env steps between buffer additions when not at episode end

# per_alpha: 0 # 0 means uniform sampling, >0 means prioritized experience replay
# per_priority: recency # recency | consistency

utd_ratio: 4  # World model updates per environment step
value_update_freq: 2  # Value updates per env step. -1 means same as utd_ratio.
pi_update_freq: 1  # Policy updates per env step.

num_rollouts: 4 # number of parallel rollouts to do in imagination for each starting state
pred_from: rollout # rollout | true_state (controls reward/termination prediction source)

# actor-critic source settings (ONLY IMAGINE SUPPORTED CURRENTLY)
ac_source: imagine # replay_rollout | imagine (controls critic/policy source)
aux_value_source: imagine # replay_true | replay_rollout | imagine

imagination_horizon: 1 
detach_imagine_value: false # if true, detach imagined latents when using them for actor/critic updates
imagine_initial_source: replay_rollout  # replay_true | replay_rollout - whether imagination starts from true encoded latents or dynamics rollout latents (head 0)
actor_source: replay_rollout # ac | replay_rollout | replay_true (controls actor source) ac copies ac source

ema_value_planning: false # If true, use EMA target network for V in planner scoring; if false, use online network

steps: 100_000
batch_size: 256
reward_coef: 0.1
value_coef: 0.1
termination_coef: 1
consistency_coef: 20
policy_coef: 1

encoder_consistency_coef: 1  # small loss to backprop consistency through the encoder, dreamer style. main loss is 20
encoder_consistency_warmup_ratio: 0.0  # Fraction of training to disable encoder consistency (0.1 = first 10%)

imagine_value_loss_coef_mult: 1 # weight for value loss on imagined rollouts, only used if ac_source or aux_value_source is imagine

rho: 0.5
lr: 3e-4
optimizer_type: adam  # 'adam' or 'adamw'
weight_decay: 0.01  # Weight decay for AdamW (ignored for Adam)
enc_lr_scale: 0.3
# Encoder LR step-change schedule
# At (1 - enc_lr_step_ratio) * steps, encoder LR is multiplied by enc_lr_step_scale
# Set enc_lr_step_scale=0.0 to freeze encoder (similar to old detaching behavior)
# Set enc_lr_step_scale=0.1 for 10× reduction instead of full freeze
# IMPORTANT: enc_lr_step_ratio controls WHEN the step happens:
#   0.0 = step at end (never freeze during training)
#   0.5 = step at halfway (default)
#   1.0 = step at start (freeze immediately)
enc_lr_step_ratio: 0.5          # Fraction of training after which to step-change LR (0.5 = halfway)
enc_lr_step_scale: 1          # Multiplier for encoder LR after step (0.0 = freeze) For tasks requiring high precision, 0 (freezing) may help.
pi_lr_scale: 1  # Policy LR = lr * pi_lr_scale. Lower than critic to stabilize training.
ensemble_lr_scaling: true  # if true, scale head LRs by ensemble size to compensate for mean-reduced gradients
grad_clip_norm: 20
tau: 0.003

# Policy trust region regularization via EMA policy
# KL(EMA_policy || current_policy) is added to policy loss to prevent rapid updates
policy_trust_region_coef: 0.0     # Coefficient for trust region KL loss (0 = disabled)
policy_ema_tau: null              # EMA tau for policy (null = inherit from tau)
td_target_use_ema_policy: false   # Use EMA policy (instead of online) to generate imagined trajectories for TD targets

discount_denom: 5
discount_min: 0.95
discount_max: 0.995
buffer_size: 1000_000

# Hot buffer (recent transitions) for mixed sampling
hot_buffer_enabled: false         # When true, also store recent transitions in a small buffer
hot_buffer_ratio: 0.01           # Fraction of each training batch drawn from the hot buffer (e.g., 0.25 for 25%)
hot_buffer_size: 50               # Capacity in transitions for the hot buffer (e.g., 500). Must be >0 when enabled

pin_memory: true
prefetch: 1

exp_name: default
data_dir: ???


# planning
eval_mpc: true
train_mpc: true
iterations: 6
extra_iter_action_dim_threshold: 15  # Add +2 iterations for action_dim >= this threshold (0 to disable)
num_samples: 512
num_elites: 64
num_pi_trajs: 24
horizon: 3
min_std: 0.05
max_std: 1
temperature: 0.5
mult_by_temp: true                  # If true, multiply scores by temp (softer). If false, divide by temp (sharper).

# reanalyze (planner distillation)
# Generate expert targets by re-running planner on observations.
# Used for policy distillation: KL(expert || policy) loss.
reanalyze_interval: 10              # 0 = disabled. >0 = steps between lazy reanalyze
reanalyze_batch_size: 64           # How many samples to reanalyze per interval
# Reanalyze planner params: null = inherit from main planner (horizon, iterations, etc.)
reanalyze_horizon: null             # Planning horizon for reanalyze (null = inherit)
reanalyze_iterations: null          # MPPI iterations for reanalyze (null = inherit)
reanalyze_num_samples: null         # Samples per MPPI iteration during reanalyze (null = inherit)
reanalyze_num_elites: null          # Number of elites for reanalyze (null = inherit)
reanalyze_num_pi_trajs: null        # Policy-seeded trajectories during reanalyze (null = inherit)
reanalyze_temperature: null         # Temperature for elite selection during reanalyze (null = inherit)
reanalyze_value_std_coef: 0.0       # Value std coef for reanalyze (0 = neutral, >0 = optimistic, <0 = pessimistic) - does NOT inherit
reanalyze_use_chosen_action: true   # If true, use chosen action as expert mean (BMPC style); if false, use planner distribution mean
reanalyze_slice_mode: false         # If true, reanalyze all timesteps in fewer slices (BMPC style); if false, sample independent t=0 obs
initial_expert_from_behavior: false # If true, use acting planner output as initial expert (no separate reanalyze); BMPC uses true
expert_std_scale: 1.0               # Multiplier for expert std before clipping (BMPC uses 1.5)
normalize_rho_weights: false        # If true, divide rho^t weights by their sum to normalize (BMPC uses true)
policy_optimization_method: distillation    # 'svg' (backprop through model) | 'distillation' (KL to expert) | 'both'
optimistic_policy_optimization_method: same  # 'same' = same as policy_optimization_method | 'svg' | 'distillation' | 'both'
policy_svg_distill_ratio: 0.5               # When method='both': ratio of distillation loss. 0=pure SVG, 1=pure distillation
                                            # Combined loss = (1 - ratio) * svg_loss + ratio * distill_loss
fix_kl_order: false                          # If true, use KL(expert || policy); if false, use KL(policy || expert)
kl_scale_min: 0.1                           # Minimum value for KL loss RunningScale (1.0 = no amplification, <1.0 allows amplifying small losses)

# actor
log_std_min: -10
log_std_max: 2
end_entropy_coeff: 1e-6
start_entropy_coeff: 1e-4
# Entropy scaling by action dimension.
# scaled_entropy = entropy * (action_dim ** entropy_action_dim_power)
# power=0: no extra scaling (entropy ~ D from Gaussian sum)
# power=1: multiply by D (entropy ~ D², original TD-MPC2 intent)
# power=0.5: compromise (entropy ~ D^1.5)
entropy_action_dim_power: 1.0
# Jacobian correction scale: controls how much tanh Jacobian affects entropy calculation.
# The Jacobian correction accounts for the change of variables when squashing actions via tanh.
# 1.0 = mathematically correct (full penalty for confidence near ±1)
# 0.5 = half penalty (more lenient on action saturation)
# 0.0 = no Jacobian penalty (legacy TD-MPC2 behavior - ignores tanh compression)
# Setting to 0.0 reproduces the old buggy behavior which may help tasks requiring
# extreme/saturated actions (e.g., dog-run) by not penalizing confidence near boundaries.
jacobian_correction_scale: 1.0

# BMPC-style policy parameterization (alternative to squashing full action):
# If true: mean = tanh(mean_raw), action = (mean + eps * std).clamp(-1, 1)
#   - No Jacobian correction needed since we only squash the mean, not the sample
#   - Policy learns in bounded mean space; noise adds exploration around tanh(mean)
# If false (default): action = mean + eps * std, then action = tanh(action)
#   - Standard squashed Gaussian; requires Jacobian correction for entropy
bmpc_policy_parameterization: true
end_dynamic_entropy_ratio: 1
start_dynamic_entropy_ratio: 1 # if set to 0.8, at 80% of training steps the entropy coeff will start to decay linearly.
dynamic_entropy_schedule : exponential  # linear | exponential

hinge_power: 4
hinge_tau: 2.5
hinge_coef: 0

# dual policy (optimistic + pessimistic)
dual_policy_enabled: true        # If true, create pessimistic + optimistic policies
optimistic_entropy_mult: 100     # Multiply entropy coeff by this for optimistic policy

# Policy multi-rollout for variance reduction
# If true, expand each state by num_rollouts and sample multiple actions per state,
# averaging the loss over rollouts for lower variance policy gradients.
pi_multi_rollout: false

# critic
num_bins: 101
vmin: -10
vmax: +10

# architecture
model_size: ???
num_enc_layers: 2 
enc_dim: 256
num_channels: 32
mlp_dim: 512
latent_dim: 512
task_dim: 96
num_q: 4
dropout: 0  # Disabled for compile + vmap + functional_call compatibility
encoder_dropout: 0.0  # Dropout for state encoder. When > 0, encodes twice (train + eval mode for stable targets)
reward_dropout_enabled: false  # Whether to apply dropout to reward heads (uses same dropout rate as value heads)
simnorm_dim: 8

reward_dim_div: 8 # reward MLP dimension is mlp_dim divided by this value
num_reward_layers: 1 # number of hidden layers in reward MLP (1 or 2)
value_dim_div: 1 # value (V) MLP dimension is mlp_dim divided by this value
num_value_layers: 2 # number of hidden layers in value (V) MLP
# logging
wandb_project: test_auxiliary
wandb_entity: thomasevers9
wandb_silent: false
enable_wandb: true
save_csv: true
log_gradients_per_loss: true
log_detail_freq: 5000
log_freq: 200

# misc
compile: true
compile_type: reduce-overhead  # 'default', 'reduce-overhead', 'max-autotune'
save_video: false
save_agent: false
seed: 1

# debug / instrumentation
debug: false  # If true, emit a log line after every agent.update (heavy). Fail-fast constraints also validated.


nvtx_profiler: false           # If true: add lightweight NVTX record_function ranges (disables compile for clarity).

# ---------------------------------------------------------------------------
# Multi-gamma auxiliary value supervision (RESEARCH FEATURE)
# ---------------------------------------------------------------------------
# Goal: improve representation learning by supervising additional discounted
#       state-value predictions at multiple discount factors (gammas) WITHOUT
#       affecting action selection, planning, or policy targets.
# Invariants:
#   * Primary (existing) Q ensemble remains unchanged and drives planning.
#   * Auxiliary heads are training-only. Gradients flow into shared trunk.
#   * When disabled or only 1 gamma is given, behavior matches baseline.
# ---------------------------------------------------------------------------
multi_gamma_gammas: [0.9, 0.95]              # Auxiliary discounts ONLY (do not include primary). Empty => feature disabled.
joint_aux_dim_mult: 1                                     # Non-empty length ≤ 6. Primary discount auto-prepended internally.
multi_gamma_head: separate          # Head architecture: 'separate' (one Linear per aux γ). Note: 'joint' is deprecated.
multi_gamma_loss_weight: 0       # Scalar weight applied to MEAN auxiliary loss (after averaging over aux gammas & time).
multi_gamma_debug_logging: true    # If True: emit richer per-gamma diagnostics (heavier overhead).
multi_gamma_log_num_examples: 4    # Max examples included in debug snapshots (targets vs preds) when debug logging.
auxiliary_value_ema: true

dtype : float32

distracted_dynamic: true    # 'dynamic' (changing distractors) or 'static' (fixed distractors)
distracted_difficulty: medium   # 'easy', 'medium', 'hard'
davis_dataset_path: /users/thomasevers/users/thomas/auxilarysignalsworldmodels/tdmpc2/tdmpc2/envs/custom_envs/distracting_control/DAVIS/JPEGImages/480p  # Path to DAVIS dataset for Distracting Control tasks
# convenience
work_dir: ???
task_title: ???
multitask: ???
tasks: ???
obs_shape: ???
action_dim: ???
episode_length: ???
obs_shapes: ???
action_dims: ???
episode_lengths: ???
seed_steps: ???
bin_size: ???

# ----------------------------------------------------------------------------
# Hydra logging and run directory configuration
# Routes Python logging to both console and a train.log file in the Hydra run dir
# ----------------------------------------------------------------------------
hydra:
    job:
        chdir: true
        env_set:
            PYTHONUNBUFFERED: "1"
    run:
        dir: logs/${now:%Y%m%d_%H%M%S}
    job_logging:
        formatters:
            simple:
                format: "%(asctime)s | %(levelname)s | %(name)s | %(message)s"
                datefmt: "%H:%M:%S"
        handlers:
            file:
                class: logging.FileHandler
                formatter: simple
                filename: train.log
                mode: a
            console:
                class: logging.StreamHandler
                formatter: simple
        root:
            level: INFO
            handlers: [file, console]
