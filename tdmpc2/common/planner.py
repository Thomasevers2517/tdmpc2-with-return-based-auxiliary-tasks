"""Planner abstraction for TD-MPC2 agents.

This module implements both the evaluation-time MPPI/CEM sampler and the
training-time particle planner used by TD-MPC2. The code is intentionally heavy
on inline comments because the planner is performance-critical and mixes
multiple tensor shapes:

* Horizon ``T`` – the number of imagined time steps.
* Action dimension ``A`` – the size of the continuous control vector.
* Population size ``N`` – either the total number of candidate trajectories
    (evaluation) or ``parents``/``children`` (training).
* Ensemble size ``H`` – number of dynamics heads in the world model.

The comments describe tensor shapes explicitly and explain why each operation
is performed so future modifications stay graph-compile friendly.
"""

from __future__ import annotations

import math as py_math
from dataclasses import dataclass, field
from typing import Any, Dict, Optional, Union

import torch
from torch import Tensor

from common import math
from common.nvtx_utils import maybe_range


@dataclass
class PlanDistribution:
    """Collection describing the action distribution produced by planning.

    Attributes:
        scores: ``(N,)`` or ``(N, 1)`` tensor of unnormalized weights for each
            candidate trajectory. The exact shape differs between evaluation
            (elites) and training (parents).
        actions: ``(T, N, A)`` tensor containing the action sequence for every
            retained candidate. ``T`` is the planning horizon, ``A`` the action
            dimension.
        action_std: ``(T, N, A)`` tensor describing the per-time-step, per-
            candidate standard deviation used when sampling perturbations.
        mean: ``(T, A)`` tensor with the population mean trajectory (mainly
            used to warm-start the evaluation planner).
        std: ``(T, A)`` tensor tracking the current exploration standard
            deviation (also used for warm-starting evaluation planner).
        extras: Dictionary holding optional tensors (e.g. value estimates,
            disagreement metrics). Stored tensors respect the same ordering so
            they can be indexed using the chosen candidate index.
    """

    scores: Tensor
    actions: Tensor
    action_std: Tensor
    mean: Tensor
    std: Tensor
    extras: Dict[str, Tensor] = field(default_factory=dict)

    def candidate_count(self) -> int:
        return int(self.actions.shape[1])

    def squeezed_scores(self) -> Tensor:
        return self.scores.squeeze(-1) if self.scores.dim() > 1 else self.scores


@dataclass
class PlanResult:
    """Result bundle returned by planner calls.

    Attributes:
        distribution: :class:`PlanDistribution` generated by the planner.
        info: Arbitrary metadata (scalars, tensors, nested structures) used for
            logging and debugging. Shapes mirror those stored in
            ``PlanDistribution`` for easy correlation.
    """

    distribution: PlanDistribution
    info: Dict[str, Any]


class Planner:
    """Handles evaluation and training-time planning for TD-MPC2.

    The planner exposes two entry points:

    * :meth:`plan_eval` implements a CEM/MPPI style sampler used during
      evaluation. It keeps ``num_samples`` candidate trajectories, rolls them
      out with the single-head world model, and refines a Gaussian proposal.
    * :meth:`plan_train` implements the latent disagreement particle planner
      used during training. Parents spawn ``particle_children`` noisy rollouts,
      the dynamics ensemble scores them, and the best children become the next
      generation of parents.

    All tensors are stored on ``self.device`` (usually CUDA) to avoid copies.
    Shapes are annotated inline so the planner stays torch.compile friendly.
    """

    def __init__(self, cfg, model, scale, discount: Tensor, device: torch.device) -> None:
        """Create a planner.

        Args:
            cfg: Hydra configuration with planner hyperparameters.
            model: TD-MPC2 world model exposing ``encode``, ``next``, ``reward``
                and friends.
            scale: Running scale object for value normalization.
            discount: ``(tasks,)`` tensor holding per-task discount factors or
                a scalar tensor for single-task setups.
            device: Torch device we operate on.
        """
        self.cfg = cfg
        self.model = model
        self.scale = scale
        self.discount = discount
        self.device = device

        self._init_eval_state()
        self._train_state_initialized = False
        if getattr(self.cfg, 'enable_train_planner', False):
            self.validate_train_planner_cfg()
            self._init_train_state()

    @staticmethod
    def _detach_nested(data: Any) -> Any:
        """Recursively move tensors to CPU without gradients for logging."""
        if isinstance(data, Tensor):
            return data.detach().cpu()
        if isinstance(data, dict):
            return {key: Planner._detach_nested(val) for key, val in data.items()}
        if isinstance(data, (list, tuple)):
            return type(data)(Planner._detach_nested(val) for val in data)
        return data

    def _init_eval_state(self) -> None:
        """Allocate the mean trajectory buffer used to warm-start evaluation."""
        self._prev_mean = torch.zeros(self.cfg.horizon, self.cfg.action_dim, device=self.device)

    def _init_train_state(self) -> None:
        """Allocate persistent storage for the particle parents and stds.

        parent actions/std tensors keep shape ``(T, parents, A)`` so we can
        reuse them across timesteps without reallocations.
        """
        parents = int(self.cfg.particle_parents)
        if parents <= 0:
            raise ValueError('particle_parents must be positive')
        horizon = int(self.cfg.horizon)
        action_dim = int(self.cfg.action_dim)
        initial_std = float(self.cfg.particle_init_gaussian_std)
        self.previous_train_parents_actions = torch.zeros(horizon, parents, action_dim, device=self.device)
        self.previous_train_parents_std = torch.full((horizon, parents, action_dim), initial_std, device=self.device)
        self._train_planner_elite_count = py_math.ceil(self.cfg.particle_child_elite_ratio * self.cfg.particle_children)
        self._train_state_initialized = True

    def validate_train_planner_cfg(self) -> None:
        """Ensure training planner config is well-defined.

        All required keys must exist and satisfy simple invariants to avoid
        silent fallback behaviour. The method raises at configuration time so
        errors surface early.
        """
        required = [
            'enable_train_planner',
            'dynamics_heads',
            'latent_disagreement_lambda',
            'particle_parents',
            'particle_children',
            'particle_iterations',
            'particle_use_policy_ratio',
            'particle_child_elite_ratio',
            'particle_init_gaussian_std',
            'min_std',
            'temperature',
        ]
        missing = [key for key in required if not hasattr(self.cfg, key)]
        if missing:
            raise AttributeError(f'Missing train planner config keys: {missing}')
        if int(self.cfg.dynamics_heads) < 2:
            raise ValueError('enable_train_planner requires cfg.dynamics_heads >= 2')
        if int(self.cfg.particle_parents) <= 0:
            raise ValueError('particle_parents must be > 0')
        if int(self.cfg.particle_children) <= 0:
            raise ValueError('particle_children must be > 0')
        if int(self.cfg.particle_iterations) <= 0:
            raise ValueError('particle_iterations must be > 0')
        use_policy_ratio = float(self.cfg.particle_use_policy_ratio)
        if not (0.0 <= use_policy_ratio <= 1.0):
            raise ValueError('particle_use_policy_ratio must be in [0, 1]')
        elite_ratio = float(self.cfg.particle_child_elite_ratio)
        if not (0.0 < elite_ratio <= 1.0):
            raise ValueError('particle_child_elite_ratio must be in (0, 1]')
        elite = py_math.ceil(elite_ratio * self.cfg.particle_children)
        if elite < 1:
            raise ValueError('particle_child_elite_ratio * particle_children must yield at least one elite child')
        if float(self.cfg.particle_init_gaussian_std) <= 0.0:
            raise ValueError('particle_init_gaussian_std must be > 0')
        if float(self.cfg.min_std) <= 0.0:
            raise ValueError('min_std must be > 0')
        if float(self.cfg.temperature) <= 0.0:
            raise ValueError('temperature must be > 0')

    def reset_states(self) -> None:
        """Reset both evaluation and training planner state buffers."""
        self.reset_eval_state()
        if getattr(self.cfg, 'enable_train_planner', False):
            self.reset_train_state()

    def reset_eval_state(self) -> None:
        """Zero the evaluation mean trajectory so the next call starts fresh."""
        self._prev_mean.zero_()

    def reset_train_state(self) -> None:
        """Reinitialize parent actions/stds to their default Gaussian."""
        if not self._train_state_initialized:
            return
        self.previous_train_parents_actions.zero_()
        self.previous_train_parents_std.fill_(float(self.cfg.particle_init_gaussian_std))

    def update_eval_mean(self, mean: Tensor) -> None:
        """Cache the latest evaluation mean trajectory for warm starting."""
        self._prev_mean.copy_(mean)

    def _collect_policy_trajectories(self, z0: Tensor, task: Optional[Tensor], *, log_detailed: bool) -> Optional[Dict[str, Any]]:
        """Optionally roll out policy trajectories to seed the candidate set.

        Returns a dictionary with actions ``(T, num_pi, A)`` and value estimates
        ``(num_pi, 1)``. The same structure is later inserted in detailed logs.
        """
        num_pi = int(self.cfg.num_pi_trajs)
        if num_pi <= 0:
            return None
        actions = torch.empty(self.cfg.horizon, num_pi, self.cfg.action_dim, device=self.device)
        z_pi = z0.repeat(num_pi, 1)
        for t in range(self.cfg.horizon - 1):
            actions[t], _ = self.model.pi(z_pi, task, use_ema=self.cfg.policy_ema_enabled, search_noise=True)
            z_pi = self.model.next(z_pi, actions[t], task)
        actions[-1], _ = self.model.pi(z_pi, task, use_ema=self.cfg.policy_ema_enabled, search_noise=True)
        values, rollout = self._value_for_sequences(z0, actions, task, log_rollout=log_detailed)
        values = values.nan_to_num(0)
        payload: Dict[str, Any] = {'actions': actions, 'values': values}
        if log_detailed and rollout is not None:
            payload['rollouts'] = [{
                'kind': 'policy',
                'iteration': -1,
                'indices': torch.arange(num_pi, device=self.device, dtype=torch.long),
                'rollout': rollout,
            }]
        else:
            payload['rollouts'] = []
        return payload

    def plan_eval(self, obs: Tensor, task: Optional[Tensor] = None, *, log_detailed: bool = False) -> PlanResult:
        """Run MPPI/CEM style planning for evaluation episodes.

        Args:
            obs: Observation tensor ``(obs_dim,)`` or batched ``(1, obs_dim)``.
            task: Optional multitask index.
            log_detailed: When True we keep per-iteration traces for debugging.

        Returns:
            :class:`PlanResult` containing elite trajectories and logging info.
        """
        with maybe_range('Agent/plan', self.cfg):
            # Encode the observation once; ``z0`` -> latent vector ``(1, L)``.
            z0 = self.model.encode(obs, task)
            horizon = int(self.cfg.horizon)
            action_dim = int(self.cfg.action_dim)
            num_samples = int(self.cfg.num_samples)
            if num_samples <= 0:
                raise ValueError('num_samples must be positive for evaluation planning')

            # Optionally seed the candidate set with trajectories from the policy
            # prior. Shapes: actions ``(T, num_pi, A)``, values ``(num_pi, 1)``.
            policy_payload = self._collect_policy_trajectories(z0, task, log_detailed=log_detailed)
            num_pi = int(self.cfg.num_pi_trajs) if policy_payload is not None else 0

            # Initial proposal distribution (mean/std) reused across iterations.
            # ``mean`` / ``std`` -> ``(T, A)``.
            mean = torch.zeros(horizon, action_dim, device=self.device)
            mean[:-1] = self._prev_mean[1:]
            std = torch.full((horizon, action_dim), float(self.cfg.max_std), dtype=torch.float, device=self.device)

            # Candidate action tensor ``actions`` -> ``(T, num_samples, A)``.
            # ``value`` stores per-trajectory returns ``(num_samples, 1)``.
            actions = torch.empty(horizon, num_samples, action_dim, device=self.device)
            value = torch.empty(num_samples, 1, device=self.device)
            if policy_payload is not None:
                actions[:, :num_pi] = policy_payload['actions']
                value[:num_pi] = policy_payload['values']

            iteration_trace: Optional[list[Dict[str, Any]]] = [] if log_detailed else None
            rollout_trace: Optional[list[Dict[str, Any]]] = [] if log_detailed else None
            if log_detailed and policy_payload is not None:
                rollout_trace.extend(policy_payload['rollouts'])

            for iteration in range(int(self.cfg.iterations)):
                # Draw Gaussian noise for trajectories that are not policy-based.
                samples_needed = num_samples - num_pi
                if samples_needed > 0:
                    noise = torch.randn(horizon, samples_needed, action_dim, device=self.device)
                    # ``sampled`` keeps the unclipped perturbations ``(T, samples_needed, A)``.
                    sampled = mean.unsqueeze(1) + std.unsqueeze(1) * noise
                    actions[:, -samples_needed:] = sampled.clamp_(-1.0, 1.0)
                if self.cfg.multitask and task is not None:
                    # Mask actions for multitask settings (broadcast mask ``(T, A)``).
                    actions = actions * self.model._action_masks[task]

                if samples_needed > 0:
                    # Evaluate only the newly drawn trajectories. Returns have
                    # shape ``(samples_needed, 1)``. ``value`` stores all
                    # trajectories contiguously.
                    sampled_value, rollout = self._value_for_sequences(z0, actions[:, -samples_needed:], task, log_rollout=log_detailed)
                    sampled_value = sampled_value.nan_to_num(0)
                    value[num_pi:] = sampled_value
                    if log_detailed and rollout is not None and rollout_trace is not None:
                        rollout_trace.append({
                            'kind': 'iteration',
                            'iteration': iteration,
                            'indices': torch.arange(num_pi, num_samples, device=self.device, dtype=torch.long),
                            'rollout': rollout,
                        })

                squeezed_value = value.squeeze(1)
                # ``elite_indices`` -> indices of the top K trajectories.
                elite_indices = torch.topk(squeezed_value, self.cfg.num_elites, dim=0).indices
                elite_value = value.index_select(0, elite_indices)
                elite_actions = actions.index_select(1, elite_indices)
                max_value = elite_value.max(0).values
                # Softmax weights after subtracting ``max_value`` for stability.
                score = torch.exp(self.cfg.temperature * (elite_value - max_value))
                score = score / score.sum(0)
                # Weighted average across elite actions; ``score`` has shape ``(K, 1)``.
                mean = (score.unsqueeze(0) * elite_actions).sum(dim=1) / (score.sum(0) + 1e-9)
                variance = (score.unsqueeze(0) * (elite_actions - mean.unsqueeze(1)) ** 2).sum(dim=1) / (score.sum(0) + 1e-9)
                std = variance.sqrt().clamp(self.cfg.min_std, self.cfg.max_std)
                if self.cfg.multitask and task is not None:
                    mask = self.model._action_masks[task]
                    mean = mean * mask
                    std = std * mask
                if log_detailed and iteration_trace is not None:
                    iteration_trace.append({
                        'iteration': iteration,
                        'mean': mean.detach().clone(),
                        'std': std.detach().clone(),
                        'scores': score.detach().clone(),
                        'elite_indices': elite_indices.detach().clone(),
                        'elite_values': elite_value.detach().clone(),
                    })
                if policy_payload is not None and num_pi == num_samples:
                    break

            squeezed_value = value.squeeze(1)
            elite_indices = torch.topk(squeezed_value, self.cfg.num_elites, dim=0).indices
            elite_value = value.index_select(0, elite_indices)
            elite_actions = actions.index_select(1, elite_indices)
            max_value = elite_value.max(0).values
            score = torch.exp(self.cfg.temperature * (elite_value - max_value))
            score = score / score.sum(0)
            # Replicate std per elite so downstream code can index ``(T, K, A)``.
            replicated_std = std.unsqueeze(1).expand(-1, elite_actions.shape[1], -1).clone()

            distribution = PlanDistribution(
                scores=score,
                actions=elite_actions,
                action_std=replicated_std,
                mean=mean,
                std=std,
                extras={
                    'values': elite_value,
                    'all_candidate_values': value,
                    'elite_indices': elite_indices,
                },
            )

            basic_info: Dict[str, Any] = {
                'elites/value_mean': float(elite_value.mean().item()),
                'elites/value_std': float(elite_value.std(unbiased=False).item()) if elite_value.shape[0] > 1 else 0.0,
                'elites/disagreement_mean': None,
                'elites/disagreement_std': None,
            }

            detailed_info: Optional[Dict[str, Any]] = None
            if log_detailed:
                converted_iterations = [self._detach_nested(entry) for entry in (iteration_trace or [])]
                converted_rollouts = [
                    {
                        'kind': entry['kind'],
                        'iteration': entry.get('iteration'),
                        'indices': self._detach_nested(entry.get('indices')),
                        'rollout': self._detach_nested(entry.get('rollout')),
                    }
                    for entry in (rollout_trace or [])
                ]
                detailed_info = {
                    'policy': self._detach_nested(policy_payload) if policy_payload is not None else None,
                    'iterations': converted_iterations,
                    'rollouts': converted_rollouts,
                    'candidate/actions': self._detach_nested(actions),
                    'candidate/values': self._detach_nested(value),
                    'distribution/score': self._detach_nested(score),
                    'distribution/mean': self._detach_nested(mean),
                    'distribution/std': self._detach_nested(std),
                    'elite/indices': self._detach_nested(elite_indices),
                    'elite/values': self._detach_nested(elite_value),
                }

            info: Dict[str, Any] = {
                'planner/type': 'eval',
                'planner/temperature': float(self.cfg.temperature),
                'basic': basic_info,
                'detailed': detailed_info,
            }
            if detailed_info is not None:
                info['planning_info'] = detailed_info

            return PlanResult(distribution=distribution, info=info)

    def plan_train(self, obs: Tensor, task: Optional[Tensor] = None, *, log_detailed: bool = False) -> PlanResult:
        """Run the latent-disagreement particle planner during training.

        Args:
            obs: Environment observation ``(obs_dim,)`` or ``(1, obs_dim)``.
            task: Optional task identifier (single-task only currently).
            log_detailed: When True, capture per-iteration diagnostics.

        Returns:
            :class:`PlanResult` describing parent distribution and log info.
        """
        if not getattr(self.cfg, 'enable_train_planner', False):
            raise RuntimeError('Train planner requested but enable_train_planner is False')
        if self.cfg.multitask:
            raise NotImplementedError('Train planner currently supports single-task configuration only')
        if not self._train_state_initialized:
            self._init_train_state()
        with maybe_range('Agent/plan_train', self.cfg):
            # The training planner imagines action sequences starting from this
            # encoded latent state. ``z0`` is shared by every particle rollout.
            z0 = self.model.encode(obs, task)
            parents = int(self.cfg.particle_parents)
            children = int(self.cfg.particle_children)
            iterations = int(self.cfg.particle_iterations)
            policy_children = min(children, int(children * float(self.cfg.particle_use_policy_ratio)))
            elite_k = self._train_planner_elite_count
            min_std = float(self.cfg.min_std)
            init_std = float(self.cfg.particle_init_gaussian_std)
            temperature = float(self.cfg.temperature)
            if policy_children > 0 and children - policy_children <= 0:
                policy_children = children - 1

            parents_actions = self.previous_train_parents_actions.clone()
            parents_std = self.previous_train_parents_std.clone()
            parents_actions = torch.cat([
                parents_actions[1:],
                parents_actions.new_zeros(1, parents, self.cfg.action_dim)
            ], dim=0)
            parents_std = torch.cat([
                parents_std[1:],
                parents_std.new_full((1, parents, self.cfg.action_dim), init_std)
            ], dim=0)
            
            
            # After the shift the parents describe step-1..T history, leaving
            # the final step free for freshly sampled children.

            iteration_payloads: Optional[list[Dict[str, Any]]] = [] if log_detailed else None
            final_parent_payload: Optional[Dict[str, Any]] = None
            used_elite_count = None

            for iteration in range(iterations):
                # Draw Gaussian noise per parent/child to explore around the
                # current parent trajectory. Resulting tensor keeps time-major
                # layout so broadcasting stays cheap during rollout.
                r = torch.randn(self.cfg.horizon, parents, children, self.cfg.action_dim, device=self.device)
                base = parents_actions.unsqueeze(2) + parents_std.unsqueeze(2) * r
                children_actions = base.clamp_(-1.0, 1.0)
                if policy_children > 0 and iteration==0:
                    # At initial iteration, replace some children with policy
                    # guided rollouts to seed the candidate set with better
                    # trajectories.
                    total_policy = parents * policy_children
                    policy_rollouts = self._sample_policy_rollouts(z0, total_policy, task)
                    policy_rollouts = policy_rollouts.view(self.cfg.horizon, parents, policy_children, self.cfg.action_dim)
                    children_actions[:, :, :policy_children, :] = policy_rollouts
                # Every parent now owns ``children`` candidate trajectories:


                flat_children = children_actions.reshape(self.cfg.horizon, parents * children, self.cfg.action_dim)
                score_dict = self.score_sequences_with_ensemble(z0, flat_children, task)
                # Raw score blends value + disagreement bonus; the rest are
                # logged for diagnostics and downstream targets.
                raw_scores = score_dict['raw'].view(parents, children) 
                value_selected = score_dict['value_selected'].view(parents, children)
                value_mean = score_dict['value_mean'].view(parents, children)
                value_max = score_dict['value_max'].view(parents, children)
                value_head = score_dict['value_max_head'].view(parents, children)
                value_mode = score_dict['value_selected_type']
                disagreement = score_dict['disagreement'].view(parents, children)
                per_step_disagreement = score_dict['per_step_disagreement'].view(self.cfg.horizon, parents, children)

                # ``best_idx`` picks the highest-scoring child for each parent.
                best_idx = raw_scores.argmax(dim=1)
                index_expanded = best_idx.view(1, parents, 1, 1).expand(self.cfg.horizon, parents, 1, self.cfg.action_dim)
                best_actions = torch.gather(children_actions, 2, index_expanded).squeeze(2)
                elite_count = min(children, max(1, elite_k))
                used_elite_count = elite_count
                _, elite_indices = torch.topk(raw_scores, elite_count, dim=1)
                elite_indices_exp = elite_indices.view(1, parents, elite_count, 1).expand(self.cfg.horizon, parents, elite_count, self.cfg.action_dim)
                elite_actions = torch.gather(children_actions, 2, elite_indices_exp)
                # Parent std is recomputed from elite spread, keeping exploration
                # targeted around promising regions.
                new_std = elite_actions.std(dim=2, unbiased=False).clamp(min=min_std)
                parents_actions = best_actions
                parents_std = new_std

                if log_detailed and iteration_payloads is not None:
                    iteration_payloads.append({
                        'iteration': iteration,
                        'raw_scores': raw_scores.detach().clone(),
                        'value_selected': value_selected.detach().clone(),
                        'disagreement': disagreement.detach().clone(),
                        'per_step_disagreement': per_step_disagreement.detach().clone(),
                        'parents_actions': parents_actions.detach().clone(),
                        'parents_std': parents_std.detach().clone(),
                        'elite_indices': elite_indices.detach().clone(),
                    })

                if iteration == iterations - 1:
                    best_raw = torch.gather(raw_scores, 1, best_idx.unsqueeze(1)).squeeze(1)
                    best_value_selected = torch.gather(value_selected, 1, best_idx.unsqueeze(1)).squeeze(1)
                    best_value_mean = torch.gather(value_mean, 1, best_idx.unsqueeze(1)).squeeze(1)
                    best_value_max = torch.gather(value_max, 1, best_idx.unsqueeze(1)).squeeze(1)
                    best_disagreement = torch.gather(disagreement, 1, best_idx.unsqueeze(1)).squeeze(1)
                    best_value_head = torch.gather(value_head, 1, best_idx.unsqueeze(1)).squeeze(1)
                    best_per_step_disagreement = torch.gather(per_step_disagreement, 2, best_idx.view(1, parents, 1).expand(self.cfg.horizon, parents, 1)).squeeze(2)
                    parents_payload = None
                    if log_detailed:
                        values_all_heads = score_dict['values_all_heads'].view(len(self.model._dynamics), parents, children)
                        parents_payload = []
                        for p in range(parents):
                            children_payload = []
                            for c in range(children):
                                # Capture per-child metrics for post-hoc analysis.
                                children_payload.append({
                                    'raw_score': raw_scores[p, c].item(),
                                    'value_selected': value_selected[p, c].item(),
                                    'value_max': value_max[p, c].item(),
                                    'value_mean': value_mean[p, c].item(),
                                    'disagreement': disagreement[p, c].item(),
                                    'value_max_head': int(value_head[p, c].item()),
                                })
                            parents_payload.append({
                                'best_child_actions': best_actions[:, p].detach().cpu(),
                                'best_child_raw_score': best_raw[p].item(),
                                'best_child_value_selected': best_value_selected[p].item(),
                                'best_child_value_max': best_value_max[p].item(),
                                'best_child_value_mean': best_value_mean[p].item(),
                                'best_child_disagreement': best_disagreement[p].item(),
                                'best_child_value_max_head': int(best_value_head[p].item()),
                                'value_selected_type': value_mode,
                                'children': children_payload,
                                'values_all_heads': values_all_heads[:, p].detach().cpu(),
                            })
                    final_parent_payload = {
                        'raw': best_raw,
                        'value_selected': best_value_selected,
                        'value_mean': best_value_mean,
                        'value_max': best_value_max,
                        'disagreement': best_disagreement,
                        'value_head': best_value_head,
                        'value_selected_type': value_mode,
                        'per_step_disagreement': best_per_step_disagreement,
                        'parents_payload': parents_payload,
                        'raw_scores_all': raw_scores.detach(),
                    }

            if final_parent_payload is None:
                raise RuntimeError('Train planner failed to finalize parent payload')

            parent_raw = final_parent_payload['raw']
            parent_value_selected = final_parent_payload['value_selected']
            parent_value_mean = final_parent_payload['value_mean']
            parent_value_max = final_parent_payload['value_max']
            parent_disagreement = final_parent_payload['disagreement']
            parent_head = final_parent_payload['value_head']
            parent_step_disagreement = final_parent_payload['per_step_disagreement']
            value_mode = final_parent_payload['value_selected_type']

            # Softmax over parent raw scores (temperature-scaled) yields the
            # sampling weights used by downstream training updates.
            logits = parent_raw / temperature
            logits = logits - logits.max()
            weights = torch.exp(logits)
            weights = weights / weights.sum().clamp_min(1e-9)

            self.previous_train_parents_actions.copy_(parents_actions.detach())
            self.previous_train_parents_std.copy_(parents_std.detach())
            if used_elite_count is None:
                used_elite_count = min(children, max(1, elite_k))

            distribution = PlanDistribution(
                scores=weights.unsqueeze(1),
                actions=parents_actions,
                action_std=parents_std,
                mean=parents_actions.mean(dim=1),
                std=parents_std.mean(dim=1),
                extras={
                    'raw_scores': parent_raw,
                    'value_selected': parent_value_selected,
                    'value_mean': parent_value_mean,
                    'value_max': parent_value_max,
                    'disagreement': parent_disagreement,
                    'value_head': parent_head,
                    'per_step_disagreement': parent_step_disagreement,
                    'weights': weights,
                },
            )

            basic_info: Dict[str, Any] = {
                'elites/value_mean': float(parent_value_selected.mean().item()),
                'elites/value_std': float(parent_value_selected.std(unbiased=False).item()) if parent_value_selected.shape[0] > 1 else 0.0,
                'elites/disagreement_mean': float(parent_disagreement.mean().item()),
                'elites/disagreement_std': float(parent_disagreement.std(unbiased=False).item()) if parent_disagreement.shape[0] > 1 else 0.0,
            }

            detailed_info: Optional[Dict[str, Any]] = None
            if log_detailed:
                converted_iterations = [self._detach_nested(entry) for entry in (iteration_payloads or [])]
                detailed_info = {
                    'iterations': converted_iterations,
                    'final': self._detach_nested(final_parent_payload),
                }
                if final_parent_payload.get('parents_payload') is not None:
                    detailed_info['parents_payload'] = final_parent_payload['parents_payload']

            info: Dict[str, Any] = {
                'planner/type': 'particle',
                'ensemble/size': len(self.model._dynamics),
                'particle/parents': parents,
                'particle/children': children,
                'particle/iterations': iterations,
                'particle/policy_children': policy_children,
                'particle/elite_k': used_elite_count,
                'planner/lambda': float(self.cfg.latent_disagreement_lambda),
                'planner/temperature': temperature,
                'parents/raw_scores': parent_raw.detach().cpu(),
                'parents/softmax': weights.detach().cpu(),
                'parents/value_selected_type': value_mode,
                'parents/value_selected': parent_value_selected.detach().cpu(),
                'parents/value_mean': parent_value_mean.detach().cpu(),
                'parents/value_max': parent_value_max.detach().cpu(),
                'parents/disagreement': parent_disagreement.detach().cpu(),
                'parents/value_max_head': parent_head.detach().cpu(),
                'parents/per_step_disagreement': parent_step_disagreement.detach().cpu(),
                'parents/value_selected_mean': parent_value_selected.mean().item(),
                'parents/value_selected_std': parent_value_selected.std(unbiased=False).item(),
                'parents/value_mean_mean': parent_value_mean.mean().item(),
                'parents/value_mean_std': parent_value_mean.std(unbiased=False).item(),
                'parents/value_max_mean': parent_value_max.mean().item(),
                'parents/value_max_std': parent_value_max.std(unbiased=False).item(),
                'parents/disagreement_mean': parent_disagreement.mean().item(),
                'parents/disagreement_std': parent_disagreement.std(unbiased=False).item(),
                'basic': basic_info,
                'detailed': detailed_info,
            }
            if detailed_info is not None and detailed_info.get('parents_payload') is not None:
                info['planning_info'] = detailed_info['parents_payload']

            return PlanResult(distribution=distribution, info=info)

    @torch._dynamo.disable()
    def sample_from_distribution(
        self,
        distribution_or_result: Union[PlanDistribution, PlanResult],
        *,
        mode: str = 'gumbel',
        temperature: float = 1.0,
        info: Optional[Dict[str, Any]] = None,
    ) -> tuple[Tensor, Tensor, Dict[str, Any], Dict[str, Any]]:
        """Sample a single trajectory from a planning distribution.

        Args:
            distribution_or_result: Either a :class:`PlanDistribution` or the
                full :class:`PlanResult` returned by ``plan_eval``/``plan_train``.
            mode: Sampling rule (``'gumbel'``/``'argmax'``/``'proportional'``).
            temperature: Temperature used by gumbel/proportional sampling.
            info: Optional logging dictionary updated in-place with selection
                metadata (falls back to ``PlanResult.info`` when omitted).

        Returns:
            Tuple ``(actions, std, sample_details, info)`` where ``actions`` and
            ``std`` are shaped ``(T, A)`` for the chosen trajectory.
        """
        if isinstance(distribution_or_result, PlanResult):
            distribution = distribution_or_result.distribution
            base_info = distribution_or_result.info
            if info is None:
                info = base_info
        else:
            distribution = distribution_or_result
        if info is None:
            info = {}

        # ``weights`` -> ``(N,)`` scores associated with each candidate.
        weights = distribution.squeezed_scores()
        if weights.numel() == 0:
            raise ValueError('Cannot sample from an empty distribution')

        if mode == 'argmax':
            idx = torch.argmax(weights)
        elif mode == 'gumbel':
            # Sample via Gumbel-Softmax to maintain differentiability w.r.t. logits.
            idx = math.gumbel_softmax_sample(weights, temperature=temperature, dim=0)
        elif mode == 'proportional':
            # Straight multinomial draw proportional to weights.
            probs = weights / weights.sum().clamp_min(1e-9)
            idx = torch.multinomial(probs, 1).squeeze(0)
        else:
            raise ValueError(f'Unknown sampling mode: {mode}')

        idx_int = int(idx.item()) if isinstance(idx, torch.Tensor) else int(idx)
        selected_score = float(weights[idx].item())
        actions = distribution.actions[:, idx].clone()
        action_std = distribution.action_std[:, idx].clone()

        def _squeeze_optional(key: str) -> Optional[Tensor]:
            # Helper to normalise extra tensors to 1-D vectors when possible.
            tensor = distribution.extras.get(key)
            if tensor is None:
                return None
            return tensor.squeeze(-1) if tensor.dim() > 1 else tensor

        selected_values = _squeeze_optional('value_selected')
        if selected_values is None:
            selected_values = _squeeze_optional('values')
        disagreements = _squeeze_optional('disagreement')

        sample_details: Dict[str, Any] = {
            'index': idx_int,
            'score': selected_score,
            'mode': mode,
        }

        if selected_values is not None:
            sample_details['value'] = float(selected_values[idx].item())
            sample_details['elites/value_mean'] = float(selected_values.float().mean().item())
        if disagreements is not None:
            sample_details['disagreement'] = float(disagreements[idx].item())
            sample_details['elites/disagreement_mean'] = float(disagreements.float().mean().item())

        basic_section = info.setdefault('basic', {})
        basic_section.setdefault('elites/value_mean', sample_details.get('elites/value_mean'))
        basic_section.setdefault('elites/disagreement_mean', sample_details.get('elites/disagreement_mean'))
        basic_section['selected/index'] = idx_int
        basic_section['selected/score'] = selected_score
        if 'value' in sample_details:
            basic_section['selected/value'] = sample_details['value']
        if 'disagreement' in sample_details:
            basic_section['selected/disagreement'] = sample_details['disagreement']

        sample_section = info.setdefault('sample', {})
        sample_section.update({
            'index': idx_int,
            'score': selected_score,
            'mode': mode,
            'actions': self._detach_nested(actions),
            'std': self._detach_nested(action_std),
        })
        if 'value' in sample_details:
            sample_section['value'] = sample_details['value']
        if 'disagreement' in sample_details:
            sample_section['disagreement'] = sample_details['disagreement']

        return actions, action_std, sample_details, info

    def _value_for_sequences(self, z0: Tensor, actions: Tensor, task: Optional[Tensor], *, log_rollout: bool = False) -> tuple[Tensor, Optional[Dict[str, Tensor]]]:
        """Evaluate sequences with the single-head model used at evaluation time.

        Args:
            z0: Latent start state ``(1, L)``.
            actions: Candidate trajectories ``(T, N, A)``.
            task: Optional task index for multitask runs.
            log_rollout: When True, also return intermediate traces for logging.

        Returns:
            Tuple ``(values, rollout_info)`` where ``values`` has shape ``(N, 1)``.
        """
        rollout = self._rollout_single_head(z0, actions, task, return_trace=log_rollout)
        values = rollout['values'].unsqueeze(1)
        if log_rollout:
            return values, rollout
        return values, None

    def _rollout_single_head(self, z0: Tensor, actions: Tensor, task: Optional[Tensor], *, return_trace: bool = False) -> dict:
        """Roll out trajectories through the EMA/non-ensemble dynamics head.

        Returns a dictionary with accumulated returns and optional traces. All
        tensors keep batch dimension ``N`` aligned with the number of sequences.
        """
        T, N, _ = actions.shape
        if z0.dim() == 1:
            z = z0.unsqueeze(0).repeat(N, 1)
        elif z0.dim() == 2 and z0.shape[0] == 1:
            z = z0.repeat(N, 1)
        else:
            raise ValueError(f'Unexpected latent shape {tuple(z0.shape)} for planner encoding')
        returns = torch.zeros(N, device=self.device)
        discount = torch.ones(N, device=self.device)
        termination = torch.zeros(N, device=self.device)
        rewards_trace = [] if return_trace else None
        discount_trace = [] if return_trace else None
        termination_trace = [] if return_trace else None
        latents_trace = [] if return_trace else None
        for t in range(T):
            if self.cfg.fix_value_est and t == T - 1:
                break
            reward_logits = self.model.reward(z, actions[t], task)
            reward = math.two_hot_inv(reward_logits, self.cfg).view(N)
            returns += discount * (1 - termination) * reward
            # Save traces before transition to capture rewards associated with current step
            if return_trace:
                rewards_trace.append(reward.detach().clone())
                discount_trace.append(discount.detach().clone())
                termination_trace.append(termination.detach().clone())
            z = self.model.next(z, actions[t], task)
            if return_trace:
                latents_trace.append(z.detach().clone())
            discount_factor = self._resolve_discount(task)
            discount = discount * discount_factor  # broadcast scalar per-task γ
            if self.cfg.episodic:
                termination = torch.maximum(termination, (self.model.termination(z, task) > 0.5).view(N).float())
        if self.cfg.fix_value_est:
            bootstrap_action = actions[-1]
        else:
            bootstrap_action, _ = self.model.pi(z, task, use_ema=self.cfg.policy_ema_enabled)
        if self.cfg.ema_value_planning:
            q_vals = self.model.Q(z, bootstrap_action, task, return_type='avg', target=True)
        else:
            q_vals = self.model.Q(z, bootstrap_action, task, return_type='avg', target=False)
        returns += discount * (1 - termination) * q_vals.view(N)
        result = {'values': returns}
        if return_trace:
            def _stack_or_empty(items):
                if not items:
                    return torch.empty(0, device=self.device)
                return torch.stack(items)

            result.update({
                'rewards': _stack_or_empty(rewards_trace),
                'discount': _stack_or_empty(discount_trace),
                'termination': _stack_or_empty(termination_trace),
                'latents': _stack_or_empty(latents_trace),
                'bootstrap_action': bootstrap_action.detach().clone(),
                'bootstrap_q': q_vals.view(N).detach().clone(),
            })
        return result

    def score_sequences_with_ensemble(self, z0: Tensor, actions: Tensor, task: Optional[Tensor]) -> dict:
        """Score trajectories using the dynamics ensemble for train-time planning.

        Args:
            z0: Latent start state ``(1, L)``.
            actions: Flattened candidate set ``(T, parents * children, A)``.
            task: Optional task index.

        Returns:
            Dictionary containing value/disagreement metrics and raw composite
            scores shaped ``(parents * children,)``.
        """
        rollout = self._rollout_with_ensemble(z0, actions, task)
        value_info = self._estimate_value_from_rollout(rollout)
        disagreement_info = self._estimate_disagreement_from_rollout(rollout)
        score = self._estimate_score(
            value=value_info['value_selected'],
            disagreement=disagreement_info['aggregate'],
            use_scale=True,
            disagreement_weight=float(self.cfg.latent_disagreement_lambda),
        )
        return {
            'raw': score,
            'value_selected': value_info['value_selected'],
            'value_selected_type': value_info['value_selected_type'],
            'value_mean': value_info['value_mean'],
            'value_max': value_info['value_max'],
            'value_max_head': value_info['value_max_head'],
            'disagreement': disagreement_info['aggregate'],
            'per_step_disagreement': disagreement_info['per_step'],
            'values_all_heads': rollout['values_all_heads'],
        }

    def _rollout_with_ensemble(self, z0: Tensor, actions: Tensor, task: Optional[Tensor]) -> dict:
        """Roll out trajectories across all dynamics heads in parallel.

        Returns a dictionary containing per-head rewards, latent states, and
        bootstrap Q-values so downstream scoring can compute statistics.
        """
        T, N, _ = actions.shape
        head_count = len(self.model._dynamics)
        if z0.dim() == 1:
            z0_expanded = z0.unsqueeze(0)
        elif z0.dim() == 2 and z0.shape[0] == 1:
            z0_expanded = z0
        else:
            raise ValueError(f'Unexpected latent shape {tuple(z0.shape)} for planner encoding')
        z0_expanded = z0_expanded.to(actions.device)
        latent_dim = z0_expanded.shape[-1]
        reward_per_head = torch.zeros(head_count, T, N, device=actions.device)
        alive_per_head = torch.ones(head_count, T, N, device=actions.device)
        bootstrap_values = torch.zeros(head_count, N, device=actions.device)
        latents_per_head = torch.zeros(head_count, T, N, latent_dim, device=actions.device)
        for head_idx in range(head_count):
            z = z0_expanded.repeat(N, 1)
            for t in range(T):
                reward_logits = self.model.reward(z, actions[t], task)
                reward = math.two_hot_inv(reward_logits, self.cfg).view(N)
                reward_per_head[head_idx, t] = reward
                z = self.model.next(z, actions[t], task, head_indices=head_idx)
                latents_per_head[head_idx, t] = z
                if self.cfg.episodic:
                    term_prob = self.model.termination(z, task)
                    alive_per_head[head_idx, t] = (term_prob <= 0.5).view(N).float()
                else:
                    alive_per_head[head_idx, t].fill_(1.0)
            bootstrap_action = actions[-1]
            if self.cfg.ema_value_planning:
                q_vals = self.model.Q(z, bootstrap_action, task, return_type='avg', target=True)
            else:
                q_vals = self.model.Q(z, bootstrap_action, task, return_type='avg', target=False)
            bootstrap_values[head_idx] = q_vals.view(N)
        optimistic_mask = alive_per_head.max(dim=0).values  # shape (T, N)
        gamma = self._resolve_discount(task).to(actions.device)
        values = torch.zeros(head_count, N, device=actions.device)
        for head_idx in range(head_count):
            discount = torch.ones(N, device=actions.device)
            for t in range(T):
                mask_t = optimistic_mask[t]
                values[head_idx] += discount * mask_t * reward_per_head[head_idx, t]
                discount = discount * mask_t * gamma
            values[head_idx] += discount * optimistic_mask[-1] * bootstrap_values[head_idx]
        return {
            'values_all_heads': values,
            'latents_per_head': latents_per_head,
            'rewards_per_head': reward_per_head,
            'alive_per_head': alive_per_head,
            'bootstrap_values': bootstrap_values,
            'optimistic_mask': optimistic_mask,
        }

    def _estimate_value_from_rollout(self, rollout: dict) -> dict:
        """Aggregate ensemble value statistics (mean/max/selected head)."""
        values = rollout['values_all_heads']  # shape (H, N)
        value_mean = values.mean(dim=0)
        value_max, value_max_head = values.max(dim=0)
        if self.cfg.train_plan_value_bootstrap == 'mean':
            value_selected = value_mean
            mode = 'mean'
        elif self.cfg.train_plan_value_bootstrap == 'max':
            value_selected = value_max
            mode = 'max'
        else:
            raise ValueError(f'Unknown train_plan_value_bootstrap: {self.cfg.train_plan_value_bootstrap}')
        return {
            'value_selected': value_selected,
            'value_selected_type': mode,
            'value_mean': value_mean,
            'value_max': value_max,
            'value_max_head': value_max_head,
        }

    def _estimate_disagreement_from_rollout(self, rollout: dict) -> dict:
        """Compute latent disagreement statistics across ensemble heads."""
        latents = rollout['latents_per_head']  # (H, T, N, L)
        latent_mean = latents.mean(dim=0)
        latent_var = (latents - latent_mean.unsqueeze(0)).pow(2).mean(dim=0)
        per_step = latent_var.mean(dim=-1)  # (T, N)
        aggregate = per_step.mean(dim=0)
        return {
            'aggregate': aggregate,
            'per_step': per_step,
        }

    def _estimate_score(self, *, value: Tensor, disagreement: Optional[Tensor], use_scale: bool, disagreement_weight: Optional[float]) -> Tensor:
        """Combine value and disagreement into a single scalar score."""
        if use_scale:
            scaled_value = self.scale(value.unsqueeze(0)).squeeze(0)
        else:
            scaled_value = value
        if disagreement is None:
            if disagreement_weight is not None:
                raise ValueError('disagreement_weight provided without disagreement tensor')
            return scaled_value
        if disagreement_weight is None:
            raise ValueError('disagreement tensor provided without weight')
        return scaled_value + disagreement_weight * disagreement

    def _resolve_discount(self, task: Optional[Tensor]) -> Tensor:
        """Return task-specific discount factor tensor for rollout loops."""
        if self.cfg.multitask:
            if task is None:
                raise RuntimeError('Task index required for multitask planning')
            if not isinstance(task, torch.Tensor):
                task_index = int(task)
            else:
                if task.numel() != 1:
                    raise RuntimeError('Task tensor must contain a single index value')
                task_index = int(task.item())
            return self.discount[task_index]
        return self.discount

    def _sample_policy_rollouts(self, z0: Tensor, total_sequences: int, task: Optional[Tensor]) -> Tensor:
        """Roll out the current policy to seed planner candidates.

        Returns an action tensor shaped ``(T, total_sequences, A)``.
        """
        if total_sequences <= 0:
            raise ValueError('total_sequences must be positive for policy rollouts')
        if z0.dim() == 1:
            base = z0.unsqueeze(0)
        elif z0.dim() == 2 and z0.shape[0] == 1:
            base = z0
        else:
            raise ValueError(f'Unexpected latent shape {tuple(z0.shape)} for policy rollouts')
        actions = torch.zeros(self.cfg.horizon, total_sequences, self.cfg.action_dim, device=self.device)
        latents = base.repeat(total_sequences, 1)
        for t in range(self.cfg.horizon):
            action_t, _ = self.model.pi(latents, task, use_ema=self.cfg.policy_ema_enabled, search_noise=True)
            actions[t] = action_t
            if t < self.cfg.horizon - 1:
                latents = self.model.next(latents, action_t, task)
        return actions
