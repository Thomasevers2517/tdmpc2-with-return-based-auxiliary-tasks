{"eval/episode_reward": 249.375244140625, "eval/episode_success": 0.0, "eval/episode_length": 500.0, "eval/step": 1100000, "eval/episode": 2199, "eval/elapsed_time": 63281.13449764252, "eval/steps_per_second": 17.382747776764013, "_timestamp": 1757585801.4227774, "_runtime": 63346.064470529556, "_step": 1101500, "train/episode_reward": 231.68692016601562, "train/episode_success": 0.0, "train/episode_length": 501, "train/episode_terminated": 0.0, "train/step": 1101500, "train/episode": 2202, "train/elapsed_time": 63343.485624313354, "train/steps_per_second": 17.389317767148693, "train/consistency_loss": 0.002378383418545127, "train/reward_loss": 0.22649729251861572, "train/value_loss": 0.3213649094104767, "train/aux_value_loss_mean": 0.30350300669670105, "train/termination_loss": 0.0, "train/total_loss": 0.13270419836044312, "train/aux_value_loss/g0_gamma0.9000": 0.363375723361969, "train/aux_value_loss/g1_gamma0.9500": 0.2436302751302719, "train/grad_norm": 0.051670175045728683, "train/pi_loss": -0.5631499290466309, "train/pi_grad_norm": 0.0030132599640637636, "train/pi_entropy": -122.24124145507812, "train/pi_scaled_entropy": 490.8367919921875, "train/pi_scale": 44.007686614990234}