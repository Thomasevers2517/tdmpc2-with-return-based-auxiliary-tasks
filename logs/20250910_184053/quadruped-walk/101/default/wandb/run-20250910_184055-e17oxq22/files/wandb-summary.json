{"eval/episode_reward": 303.9207763671875, "eval/episode_success": 0.0, "eval/episode_length": 500.0, "eval/step": 975000, "eval/episode": 1949, "eval/elapsed_time": 55972.58533740044, "eval/steps_per_second": 17.41924183281405, "_timestamp": 1757578807.3644183, "_runtime": 56352.00611138344, "_step": 980500, "train/episode_reward": 218.02621459960938, "train/episode_success": 0.0, "train/episode_length": 501, "train/episode_terminated": 0.0, "train/step": 980500, "train/episode": 1960, "train/elapsed_time": 56349.427252054214, "train/steps_per_second": 17.400354321511866, "train/consistency_loss": 0.002293048892170191, "train/reward_loss": 0.24519166350364685, "train/value_loss": 0.3222688138484955, "train/aux_value_loss_mean": 0.44511088728904724, "train/termination_loss": 0.0, "train/total_loss": 0.14711812138557434, "train/aux_value_loss/g0_gamma0.9000": 0.4384598135948181, "train/aux_value_loss/g1_gamma0.9500": 0.45176196098327637, "train/grad_norm": 0.03508732467889786, "train/pi_loss": -0.5716554522514343, "train/pi_grad_norm": 0.004512972664088011, "train/pi_entropy": -120.76502990722656, "train/pi_scaled_entropy": 493.25616455078125, "train/pi_scale": 42.32556915283203}