{"eval/episode_reward": 202.5819091796875, "eval/episode_success": 0.0, "eval/episode_length": 500.0, "eval/step": 1150000, "eval/episode": 2299, "eval/elapsed_time": 62511.198791742325, "eval/steps_per_second": 18.396703666350323, "_timestamp": 1757585819.3370783, "_runtime": 63364.05828142166, "_step": 1171500, "train/episode_reward": 89.83785247802734, "train/episode_success": 0.0, "train/episode_length": 501, "train/episode_terminated": 0.0, "train/step": 1171500, "train/episode": 2342, "train/elapsed_time": 63361.31767320633, "train/steps_per_second": 18.489198820677203, "train/consistency_loss": 0.003721607616171241, "train/reward_loss": 0.1128888949751854, "train/value_loss": 0.4096171259880066, "train/aux_value_loss_mean": 0.5797967314720154, "train/termination_loss": 0.0, "train/total_loss": 0.18466243147850037, "train/aux_value_loss/g0_gamma0.9000": 0.543800950050354, "train/aux_value_loss/g1_gamma0.9500": 0.6157925128936768, "train/grad_norm": 0.2952643632888794, "train/pi_loss": -0.10908006876707077, "train/pi_grad_norm": 7.066395801835768e-10, "train/pi_entropy": -10.37522029876709, "train/pi_scaled_entropy": 3.440290689468384, "train/pi_scale": 48.22643280029297}