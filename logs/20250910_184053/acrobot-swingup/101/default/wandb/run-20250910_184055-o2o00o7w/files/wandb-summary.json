{"eval/episode_reward": 220.7654571533203, "eval/episode_success": 0.0, "eval/episode_length": 500.0, "eval/step": 1025000, "eval/episode": 2049, "eval/elapsed_time": 55185.66974282265, "eval/steps_per_second": 18.57366241592655, "_timestamp": 1757578825.4792378, "_runtime": 56370.20044088364, "_step": 1044000, "train/episode_reward": 213.41348266601562, "train/episode_success": 0.0, "train/episode_length": 501, "train/episode_terminated": 0.0, "train/step": 1044000, "train/episode": 2087, "train/elapsed_time": 56367.45985078812, "train/steps_per_second": 18.521324231455555, "train/consistency_loss": 0.0036691133864223957, "train/reward_loss": 0.11567787826061249, "train/value_loss": 0.40924856066703796, "train/aux_value_loss_mean": 0.4851589500904083, "train/termination_loss": 0.0, "train/total_loss": 0.17439080774784088, "train/aux_value_loss/g0_gamma0.9000": 0.5223944783210754, "train/aux_value_loss/g1_gamma0.9500": 0.4479234218597412, "train/grad_norm": 0.24063782393932343, "train/pi_loss": -0.08530112355947495, "train/pi_grad_norm": 7.159068338147279e-10, "train/pi_entropy": -10.369047164916992, "train/pi_scaled_entropy": 3.4464638233184814, "train/pi_scale": 46.69329833984375}