{"eval/episode_reward": 133.6722869873047, "eval/episode_success": 0.0, "eval/episode_length": 500.0, "eval/step": 50000, "eval/episode": 99, "eval/elapsed_time": 6650.187180042267, "eval/steps_per_second": 7.518585364041169, "_timestamp": 1757519350.4711826, "_runtime": 7725.141336679459, "_step": 58500, "train/episode_reward": 23.05394744873047, "train/episode_success": 0.0, "train/episode_length": 501, "train/episode_terminated": 0.0, "train/step": 58500, "train/episode": 116, "train/elapsed_time": 7720.538848161697, "train/steps_per_second": 7.577191326992567, "train/consistency_loss": 0.005075772758573294, "train/reward_loss": 0.1906323879957199, "train/value_loss": 0.33774882555007935, "train/aux_value_loss_mean": 0.580886721611023, "train/termination_loss": 0.0, "train/total_loss": 0.21244226396083832, "train/aux_value_loss/g0_gamma0.9000": 0.5913540720939636, "train/aux_value_loss/g1_gamma0.9500": 0.570419430732727, "train/grad_norm": 0.032326098531484604, "train/pi_loss": -0.27210891246795654, "train/pi_grad_norm": 0.0035440116189420223, "train/pi_entropy": -113.18463134765625, "train/pi_scaled_entropy": 491.92852783203125, "train/pi_scale": 77.0880126953125}