{"eval/episode_reward": 7.384069919586182, "eval/episode_success": 0.0, "eval/episode_length": 500.0, "eval/step": 50000, "eval/episode": 99, "eval/elapsed_time": 6260.114482164383, "eval/steps_per_second": 7.987074380581122, "_timestamp": 1757519344.8591895, "_runtime": 7719.989601612091, "_step": 62500, "train/episode_reward": 30.809724807739258, "train/episode_success": 0.0, "train/episode_length": 501, "train/episode_terminated": 0.0, "train/step": 62500, "train/episode": 124, "train/elapsed_time": 7715.722945928574, "train/steps_per_second": 8.100342694780137, "train/consistency_loss": 0.004234395921230316, "train/reward_loss": 0.04240141063928604, "train/value_loss": 0.27133581042289734, "train/aux_value_loss_mean": 0.3861987292766571, "train/termination_loss": 0.0, "train/total_loss": 0.1546815186738968, "train/aux_value_loss/g0_gamma0.9000": 0.4000796973705292, "train/aux_value_loss/g1_gamma0.9500": 0.37231776118278503, "train/grad_norm": 0.08871401101350784, "train/pi_loss": -0.13207599520683289, "train/pi_grad_norm": 0.0018707403214648366, "train/pi_entropy": -9.275176048278809, "train/pi_scaled_entropy": 3.055272340774536, "train/pi_scale": 1.8450522422790527}