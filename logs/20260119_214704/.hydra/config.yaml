task: quadruped-walk
obs: state
episodic: false
checkpoint: ???
eval_episodes: 5
eval_freq: 10000
eval_mean_head_reduce: false
knn_entropy_k: 5
knn_entropy_dim: 512
critic_type: V
pi_rollout_horizon: 1
final_rho: 0.125
sac_style_td: false
n_mc_samples_target: 1
num_reward_heads: 2
value_std_coef_default: 1.0
td_target_std_coef: 0
td_target_dynamics_reduction: mean
local_td_bootstrap: true
td_target_use_all_dynamics_heads: true
policy_value_std_coef: 0
optimistic_policy_value_std_coef: 1
planner_value_std_coef_train: 0.3
planner_value_std_coef_eval: -1
planner_use_all_heads_eval: true
policy_seed_noise_std: 0.05
planner_action_noise_std: 0.0
planner_lambda_disagreement: 0
planner_policy_elites_first_iter_only: false
planner_num_dynamics_heads: 2
prior_hidden_div: 16
prior_scale: 0.1
prior_logit_scale: 1
dynamics_dropout: 0.0
dynamics_num_layers: 2
train_act_std_coeff: 0
best_eval: true
buffer_update_interval: 1
utd_ratio: 4
value_update_freq: 1
pi_update_freq: 1
num_rollouts: 4
pred_from: rollout
ac_source: imagine
aux_value_source: imagine
imagination_horizon: 1
detach_imagine_value: false
imagine_initial_source: replay_rollout
actor_source: replay_rollout
ema_value_planning: true
steps: 100000
batch_size: 256
reward_coef: 0.1
value_coef: 0.1
termination_coef: 1
consistency_coef: 20
policy_coef: 1
encoder_consistency_coef: 1
encoder_consistency_warmup_ratio: 0.0
imagine_value_loss_coef_mult: 1
rho: 0.5
lr: 0.0003
enc_lr_scale: 0.3
enc_lr_step_ratio: 0.5
enc_lr_step_scale: 0.0
pi_lr_scale: 1
ensemble_lr_scaling: true
grad_clip_norm: 20
tau: 0.003
discount_denom: 5
discount_min: 0.95
discount_max: 0.995
buffer_size: 1000000
hot_buffer_enabled: false
hot_buffer_ratio: 0.01
hot_buffer_size: 50
pin_memory: true
prefetch: 1
exp_name: default
data_dir: ???
eval_mpc: true
train_mpc: true
iterations: 6
num_samples: 512
num_elites: 64
num_pi_trajs: 24
horizon: 3
min_std: 0.05
max_std: 1
temperature: 0.5
reanalyze_interval: 5
reanalyze_batch_size: 32
reanalyze_horizon: 3
reanalyze_iterations: 4
reanalyze_num_samples: 512
reanalyze_num_elites: 64
reanalyze_num_pi_trajs: 24
reanalyze_temperature: 0.5
reanalyze_value_std_coef: 0
policy_optimization_method: distillation
optimistic_policy_optimization_method: distillation
fix_kl_order: true
log_std_min: -10
log_std_max: 2
end_entropy_coeff: 1.0e-06
start_entropy_coeff: 0
entropy_action_dim_power: 1
jacobian_correction_scale: 1
end_dynamic_entropy_ratio: 1
start_dynamic_entropy_ratio: 1
dynamic_entropy_schedule: exponential
hinge_power: 4
hinge_tau: 2.5
hinge_coef: 0
dual_policy_enabled: true
optimistic_entropy_mult: 100
pi_multi_rollout: false
num_bins: 101
vmin: -10
vmax: 10
model_size: ???
num_enc_layers: 2
enc_dim: 256
num_channels: 32
mlp_dim: 512
latent_dim: 512
task_dim: 96
num_q: 2
dropout: 0
encoder_dropout: 0.0
reward_dropout_enabled: false
simnorm_dim: 8
reward_dim_div: 32
num_reward_layers: 1
value_dim_div: 1
num_value_layers: 2
wandb_project: tdmpc2-tdmpc2
wandb_entity: thomasevers9
wandb_silent: false
enable_wandb: true
save_csv: true
log_gradients_per_loss: true
log_detail_freq: 5000
log_freq: 200
compile: true
compile_type: reduce-overhead
save_video: false
save_agent: false
seed: 1
debug: false
nvtx_profiler: false
multi_gamma_gammas:
- 0.9
- 0.95
joint_aux_dim_mult: 1
multi_gamma_head: separate
multi_gamma_loss_weight: 0
multi_gamma_debug_logging: true
multi_gamma_log_num_examples: 4
auxiliary_value_ema: true
dtype: float32
distracted_dynamic: true
distracted_difficulty: medium
davis_dataset_path: /users/thomasevers/users/thomas/auxilarysignalsworldmodels/tdmpc2/tdmpc2/envs/custom_envs/distracting_control/DAVIS/JPEGImages/480p
work_dir: ???
task_title: ???
multitask: ???
tasks: ???
obs_shape: ???
action_dim: ???
episode_length: ???
obs_shapes: ???
action_dims: ???
episode_lengths: ???
seed_steps: ???
bin_size: ???
