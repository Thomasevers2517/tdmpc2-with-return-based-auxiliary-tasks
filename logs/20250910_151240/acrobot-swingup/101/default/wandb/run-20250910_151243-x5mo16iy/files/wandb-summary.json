{"eval/episode_reward": 4.103763580322266, "eval/episode_success": 0.0, "eval/episode_length": 500.0, "eval/step": 25000, "eval/episode": 49, "eval/elapsed_time": 1659.7727706432343, "eval/steps_per_second": 15.062302769499833, "_timestamp": 1757512316.6700404, "_runtime": 2353.324999332428, "_step": 35000, "train/episode_reward": 1.3866803646087646, "train/episode_success": 0.0, "train/episode_length": 501, "train/episode_terminated": 0.0, "train/step": 35000, "train/episode": 69, "train/elapsed_time": 2350.3712549209595, "train/steps_per_second": 14.891264487140358, "train/consistency_loss": 0.004507529549300671, "train/reward_loss": 0.026675637811422348, "train/value_loss": 0.20259486138820648, "train/aux_value_loss_mean": 0.4467869699001312, "train/termination_loss": 0.0, "train/total_loss": 0.15775634348392487, "train/aux_value_loss/g0_gamma0.9000": 0.4460388422012329, "train/aux_value_loss/g1_gamma0.9500": 0.44753509759902954, "train/grad_norm": 0.07857248187065125, "train/pi_loss": -0.05098738521337509, "train/pi_grad_norm": 3.1105020781296844e-08, "train/pi_entropy": -10.384190559387207, "train/pi_scaled_entropy": 3.4272985458374023, "train/pi_scale": 1.0143660306930542}