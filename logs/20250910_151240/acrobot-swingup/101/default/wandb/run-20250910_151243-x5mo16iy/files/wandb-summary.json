{"eval/episode_reward": 15.203557968139648, "eval/episode_success": 0.0, "eval/episode_length": 500.0, "eval/step": 50000, "eval/episode": 99, "eval/elapsed_time": 3869.0492038726807, "eval/steps_per_second": 12.923071629575832, "_timestamp": 1757514379.856311, "_runtime": 4416.511270046234, "_step": 56000, "train/episode_reward": 36.5556755065918, "train/episode_success": 0.0, "train/episode_length": 501, "train/episode_terminated": 0.0, "train/step": 56000, "train/episode": 111, "train/elapsed_time": 4413.557506561279, "train/steps_per_second": 12.68817726216308, "train/consistency_loss": 0.005030638538300991, "train/reward_loss": 0.03551418334245682, "train/value_loss": 0.22481322288513184, "train/aux_value_loss_mean": 0.47040724754333496, "train/termination_loss": 0.0, "train/total_loss": 0.17368625104427338, "train/aux_value_loss/g0_gamma0.9000": 0.4736395478248596, "train/aux_value_loss/g1_gamma0.9500": 0.4671749770641327, "train/grad_norm": 0.08656566590070724, "train/pi_loss": -0.19816458225250244, "train/pi_grad_norm": 1.5557741761540456e-08, "train/pi_entropy": -10.393810272216797, "train/pi_scaled_entropy": 3.4159255027770996, "train/pi_scale": 1.15234375}