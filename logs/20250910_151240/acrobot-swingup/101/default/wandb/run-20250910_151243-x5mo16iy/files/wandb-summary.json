{"eval/episode_reward": 49.31426239013672, "eval/episode_success": 0.0, "eval/episode_length": 500.0, "eval/step": 100000, "eval/episode": 199, "eval/elapsed_time": 8628.167510986328, "eval/steps_per_second": 11.589946517921568, "_timestamp": 1757519342.819025, "_runtime": 9379.473984003067, "_step": 108000, "train/episode_reward": 25.73037338256836, "train/episode_success": 0.0, "train/episode_length": 501, "train/episode_terminated": 0.0, "train/step": 108000, "train/episode": 215, "train/elapsed_time": 9376.52026128769, "train/steps_per_second": 11.518132205813442, "train/consistency_loss": 0.004135933704674244, "train/reward_loss": 0.05229005217552185, "train/value_loss": 0.2906138002872467, "train/aux_value_loss_mean": 0.5353803634643555, "train/termination_loss": 0.0, "train/total_loss": 0.17054708302021027, "train/aux_value_loss/g0_gamma0.9000": 0.530826210975647, "train/aux_value_loss/g1_gamma0.9500": 0.5399345755577087, "train/grad_norm": 0.12011437118053436, "train/pi_loss": -0.10159486532211304, "train/pi_grad_norm": 0.001061648246832192, "train/pi_entropy": -9.96535873413086, "train/pi_scaled_entropy": 3.3982577323913574, "train/pi_scale": 5.132268905639648}