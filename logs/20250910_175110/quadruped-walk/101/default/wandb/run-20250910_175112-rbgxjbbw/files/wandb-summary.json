{"eval/episode_reward": 108.24825286865234, "eval/episode_success": 0.0, "eval/episode_length": 500.0, "eval/step": 0, "eval/episode": 0, "eval/elapsed_time": 72.98889541625977, "eval/steps_per_second": 0.0, "_timestamp": 1757520347.1621027, "_runtime": 874.6008408069611, "_step": 13500, "train/episode_reward": 9.563896179199219, "train/episode_success": 0.0, "train/episode_length": 501, "train/episode_terminated": 0.0, "train/step": 13500, "train/episode": 26, "train/elapsed_time": 871.2135083675385, "train/steps_per_second": 15.495627501570787, "train/consistency_loss": 0.004726463463157415, "train/reward_loss": 0.14461375772953033, "train/value_loss": 0.32636064291000366, "train/aux_value_loss_mean": 0.3927260935306549, "train/termination_loss": 0.0, "train/total_loss": 0.18089932203292847, "train/aux_value_loss/g0_gamma0.9000": 0.3582392632961273, "train/aux_value_loss/g1_gamma0.9500": 0.4272129237651825, "train/grad_norm": 0.058183617889881134, "train/pi_loss": -0.7995634078979492, "train/pi_grad_norm": 0.018560225144028664, "train/pi_entropy": -110.49173736572266, "train/pi_scaled_entropy": 492.7488098144531, "train/pi_scale": 36.261940002441406}