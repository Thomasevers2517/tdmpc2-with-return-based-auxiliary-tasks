{"eval/episode_reward": 40.87333679199219, "eval/episode_success": 0.0, "eval/episode_length": 500.0, "eval/step": 25000, "eval/episode": 49, "eval/elapsed_time": 1608.0087769031525, "eval/steps_per_second": 15.547178820844026, "_timestamp": 1757522303.3135395, "_runtime": 2830.752277612686, "_step": 43000, "train/episode_reward": 13.704504013061523, "train/episode_success": 0.0, "train/episode_length": 501, "train/episode_terminated": 0.0, "train/step": 43000, "train/episode": 85, "train/elapsed_time": 2827.364933490753, "train/steps_per_second": 15.20850721838403, "train/consistency_loss": 0.0051590087823569775, "train/reward_loss": 0.17613673210144043, "train/value_loss": 0.35308924317359924, "train/aux_value_loss_mean": 0.5304896235466003, "train/termination_loss": 0.0, "train/total_loss": 0.2091517448425293, "train/aux_value_loss/g0_gamma0.9000": 0.49936437606811523, "train/aux_value_loss/g1_gamma0.9500": 0.5616148114204407, "train/grad_norm": 0.04235701635479927, "train/pi_loss": -0.2857248783111572, "train/pi_grad_norm": 0.005242213606834412, "train/pi_entropy": -111.9325942993164, "train/pi_scaled_entropy": 493.11749267578125, "train/pi_scale": 72.89521789550781}