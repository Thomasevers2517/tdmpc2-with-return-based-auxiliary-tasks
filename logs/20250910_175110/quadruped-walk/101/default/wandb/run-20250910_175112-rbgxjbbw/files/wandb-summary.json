{"eval/episode_reward": 108.24825286865234, "eval/episode_success": 0.0, "eval/episode_length": 500.0, "eval/step": 0, "eval/episode": 0, "eval/elapsed_time": 72.98889541625977, "eval/steps_per_second": 0.0, "_timestamp": 1757519972.1595597, "_runtime": 499.59829783439636, "_step": 7000, "train/episode_reward": 168.82119750976562, "train/episode_success": 0.0, "train/episode_length": 501, "train/episode_terminated": 0.0, "train/step": 7000, "train/episode": 13, "train/elapsed_time": 496.2109184265137, "train/steps_per_second": 14.106904423217895, "train/consistency_loss": 0.0049911728128790855, "train/reward_loss": 0.16906780004501343, "train/value_loss": 0.3557650148868561, "train/aux_value_loss_mean": 0.5716595649719238, "train/termination_loss": 0.0, "train/total_loss": 0.20947270095348358, "train/aux_value_loss/g0_gamma0.9000": 0.5576096177101135, "train/aux_value_loss/g1_gamma0.9500": 0.5857095718383789, "train/grad_norm": 0.1311645656824112, "train/pi_loss": -0.41888171434402466, "train/pi_grad_norm": 0.006105333101004362, "train/pi_entropy": -113.23463439941406, "train/pi_scaled_entropy": 492.46533203125, "train/pi_scale": 40.22742462158203}