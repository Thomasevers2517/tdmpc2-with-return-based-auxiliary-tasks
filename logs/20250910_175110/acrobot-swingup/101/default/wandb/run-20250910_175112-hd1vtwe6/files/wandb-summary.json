{"eval/episode_reward": 12.831108093261719, "eval/episode_success": 0.0, "eval/episode_length": 500.0, "eval/step": 25000, "eval/episode": 49, "eval/elapsed_time": 1546.9540340900421, "eval/steps_per_second": 16.160790462468807, "_timestamp": 1757522240.9758358, "_runtime": 2768.431535720825, "_step": 44000, "train/episode_reward": 0.06322406977415085, "train/episode_success": 0.0, "train/episode_length": 501, "train/episode_terminated": 0.0, "train/step": 44000, "train/episode": 87, "train/elapsed_time": 2764.965508699417, "train/steps_per_second": 15.913399231043824, "train/consistency_loss": 0.003669789992272854, "train/reward_loss": 0.04138883575797081, "train/value_loss": 0.25867387652397156, "train/aux_value_loss_mean": 0.4243171811103821, "train/termination_loss": 0.0, "train/total_loss": 0.1458337903022766, "train/aux_value_loss/g0_gamma0.9000": 0.43690556287765503, "train/aux_value_loss/g1_gamma0.9500": 0.41172879934310913, "train/grad_norm": 0.07010304182767868, "train/pi_loss": -0.11660921573638916, "train/pi_grad_norm": 2.747803762304102e-08, "train/pi_entropy": -10.391608238220215, "train/pi_scaled_entropy": 3.4239025115966797, "train/pi_scale": 1.6117782592773438}