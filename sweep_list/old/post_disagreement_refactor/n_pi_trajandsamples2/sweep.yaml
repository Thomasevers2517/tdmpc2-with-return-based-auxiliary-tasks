program: tdmpc2/train.py
method: grid
command:
  - ${env}
  - python
  - ${program}
  - ${args_no_hyphens}
metric:
  name: eval/episode_reward
  goal: maximize
parameters:
  # Base task (matches config default). Add more tasks if desired.
  task:
    values:
      - acrobot-swingup
      - quadruped-walk
      # - walker-run
  reward_dim_div: 1 # reward MLP dimension is mlp_dim divided by this value


  #THIS VERSION ALSO SET REDUCE ENTROPY COEFF at 0.5, unlike the previous one
    # Keep MPC defaults from config; override as needed
  iterations:
    value: 6
  num_samples:
    value: 2048
  num_elites:
    values : [64]
  num_pi_trajs:
    values: [1024]
  horizon:
    values: [3, 6]
  final_rho:
    value: 0.125
  min_std:
    values: [0, 0.05]
  max_std:
    values: [0.5]

# Hot buffer (recent transitions) for mixed sampling
  hot_buffer_enabled:          # When true, also store recent transitions in a small buffer
    value: true         # When true, also store recent transitions in a small buffer
  hot_buffer_ratio: 
    value: 0.01      #0.01 is 3 samples     # Fraction of each training batch drawn from the hot buffer (e.g., 0.25 for 25%)
  hot_buffer_size: 
    value: 50              # Capacity in transitions for the hot buffer (e.g., 500). Must be >0 when enabled


  sac_style_td: 
    value: false

  n_mc_samples_target:
    value: 1

  # UTD sweep (align with config key `utd_ratio`)
  utd_ratio:
    values: [2]

  # Core: latent disagreement refactor knobs
  planner_num_dynamics_heads:
    values: [4]
  planner_lambda_disagreement:
    values: [0]
  fix_value_est:
    values: [true]

    # Training noise multiplier (planner applies to first action in training)
  train_act_std_coeff:
    values: [0.0]


  buffer_update_interval:
    value: 10




  planner_temperature_train:
    value: 0.5
  planner_temperature_eval:
    value: 0.5



  # Common training/eval settings from config
  eval_mpc:
    value: true
  train_mpc:
    value: true
  obs:
    values: [state]
  steps:
    value: 100000
  eval_freq:
    value: 10000
  batch_size:
    value: 256

  # Repro and logging
  seed:
    values: [101, 202, 303, 404]
  enable_wandb:
    value: true
  wandb_project:
    value: test_auxiliary
  wandb_entity:
    value: thomasevers9
  save_video:
    value: false

  # Compilation on to validate advanced analysis path
  compile:
    value: true
  compile_type:
    value: reduce-overhead
