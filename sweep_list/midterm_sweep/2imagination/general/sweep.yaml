# W&B sweep: 1M-step runs over selected control tasks, 3 seeds each.
# Requested (interpreted) tasks:
#   acrobot-swingup, finger-turn-easy, finger-turn-hard, walker-walk, walker-run
# Updated: removed finger-spin (too easy), added quadruped-walk, reacher-hard per user request.
# If you intended another task (e.g., fish-swim, hopper-walk variant, etc.), let me know to add it.
#
# Sweep is a full grid: tasks x seeds. All other hyperparameters fixed.
# Steps fixed to 1_000_000 as requested.
#
# Launch:
#   wandb sweep wandb_sweeps/one_million_task_sweep.yaml
#   wandb agent <entity>/<project>/<sweep_id>
# To run multiple agents in parallel (e.g., 2 GPUs): start multiple wandb agent commands.
#
# Each run calls tdmpc2/train.py with hydra overrides from these parameters.

program: tdmpc2/train.py
method: random
command:
  - ${env}
  - /space/thomasevers/conda-envs/tdmpc2/bin/python
  - ${program}
  - ${args_no_hyphens}
metric:
  name: eval/episode_reward
  goal: maximize
parameters:
  task:
    values:
    values: 
      # - cartpole_balance
      # - cartpole_balance_sparse
      # - cartpole_swingup
      # - cup_catch
      # - finger_spin
      # - reacher_easy
      - reacher_hard
      # - walker_stand
      # - walker_walk
      # - hopper_stand
      # - hopper_hop
      - cheetah_run
      - acrobot_swingup
      # - cartpole_swingup_sparse
      # - finger_turn_easy
      # - finger_turn_hard
      - quadruped_walk
      # - quadruped_run
      - walker_run

      # - humanoid-run
  search_pi_std_mult:
    values: [1, 3, 10]

  buffer_update_interval:
    values: [100]

  policy_coef:
    values: [1] #seems it shouldnt increase

  ema_value_planning:
    values: [false] # false seems better, quicker model fixing

  detach_imagine_value:
    values: [false]
  imagination_horizon:
    values: [1]
  pred_from: 
    values: ["rollout"] # rollout, true_state or both
  split_batch: 
    values: [true] 
  ac_source:
    values: ["imagine"] # replay_rollout | imagine (controls critic/actor source)
  actor_source:
    values: ["replay_rollout"] # ac | replay_rollout | replay_true (controls actor source) ac copies ac source
  aux_value_source:
    values: ["replay_true"] # replay_true | replay_rollout | imagine

  log_std_max:
    values: [2]
  reward_dim_div:
    values: [1]
  # num_enc_layers:
  #   values: [2] # originally 2
  entropy_coef:
    values: [1e-4, 3e-5, 1e-5] #originally 1e-4
    values: [1e-4, 3e-5, 1e-5] #originally 1e-4
  hinge_coef:
    values: [0.01, 0] #[0.01]

  imagine_value_loss_coef_mult:
    values: [1] #[0.1, 0.01, 0] # 0 to disable
  num_rollouts:
    values: [4]


  eval_mpc:
    values: [true]
  train_mpc:
    values: [true]

  utd_ratio:
    values: [1,2]
    values: [1,2]
  ac_utd_multiplier:
    values: [1]

  lr:
    values: [3e-4]
  enc_lr_scale:
    values: [0.1]
  horizon:
    values: [3]
  rho:
    values: [0.5]


  encoder_consistency_coef:
    values: [1]  # small loss to backprop consistency through the encoder, dreamer style. main loss is 20

  multi_gamma_head: 
    values:
        # - joint 
        - separate
  seed:
    values: [102, 203, 304]
  best_eval: 
    values: [false]

  # Multi-gamma auxiliary value experiments (mirroring earlier setup)
  multi_gamma_gammas:
    values:
      # - [0.97]
      - [0.95]
      # - [0.8, 0.95]
      # - [0.9, 0.95]




  joint_aux_dim_mult:
    values: [1]
  multi_gamma_loss_weight:
    values: [0.1]
  # policy_ema_enabled:
  #   values: [false]
  # policy_ema_tau:
  #   values: [0.05] # 0.99 is half life of ~70 updates at tau=0.01. 0.05 is half life of ~14 updates.
  # encoder_ema_enabled:
  #   values: [false]
  # encoder_ema_tau:
  #   values: [0.05, 0.01]
  steps:
    value: 100000
  eval_freq:
    value: 10000

  batch_size:
    value: 256
  model_size:
    values: [5]
  obs:
    values:
      # - rgb
      - state
  compile:
    value: true
  enable_wandb:
    value: true
  wandb_project:
    value: tdmpc2-tdmpc2
  wandb_entity:
    value: thomasevers9
  save_video:
    value: false

