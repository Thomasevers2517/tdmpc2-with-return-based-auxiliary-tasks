program: tdmpc2/train.py
project: tdmpc2-tdmpc2
method: grid
command:
  - ${env}
  - python
  - ${program}
  - ${args_no_hyphens}
parameters:
  #############################################
  # Sweep 109: Dog-Run Old Config Replication
  # 
  # Replicate the config from successful sweep 44 (8skgxqos)
  # commit 95b2701 to understand what made it work.
  #
  # Key differences from current defaults:
  # - td_bootstrap_mode: min (not local - feature didn't exist back then)
  # - num_q: 2 (not 8)
  # - hot_buffer_enabled: true (not false)
  # - temperature: 0.1 (not 0.5)
  # - dynamics_prior_enabled: false (was disabled back then)
  # - Higher entropy: 3e-3 / 5e-4 (not 3e-5)
  # - start_dynamic_entropy_ratio: 1.0 (no decay)
  # - num_reward_heads: 1 (not 2)
  # - policy_lambda_value_disagreement: 1.0 (was being tested)
  #
  # Total: 1 config Ã— 2 seeds = 2 runs
  #############################################

  task:
    value: dog-run

  # Jacobian correction = 1.0 (same as old use_scaled_entropy: false)
  jacobian_correction_scale:
    value: 1.0

  # Entropy: 3e-5 is equivalent to old 3e-3 / action_dim (38 for dog)
  # Back then entropy wasn't scaled by action_dim, now it is
  start_entropy_coeff:
    value: 3e-5

  end_entropy_coeff:
    value: 1e-6

  # No entropy decay (like old sweep)
  start_dynamic_entropy_ratio:
    value: 1.0

  # TD bootstrap: min (old behavior, td_bootstrap_mode didn't exist)
  td_bootstrap_mode:
    value: min

  # Value disagreement penalty (tested in old sweep, use moderate value)
  policy_lambda_value_disagreement:
    value: 1.0

  optimistic_policy_lambda_value_disagreement:
    value: 0.0

  # Old config values
  num_q:
    value: 2

  num_reward_heads:
    value: 1

  hot_buffer_enabled:
    value: true

  hot_buffer_ratio:
    value: 0.01

  hot_buffer_size:
    value: 5

  temperature:
    value: 0.1

  # Dynamics prior was disabled back then
  dynamics_prior_enabled:
    value: false

  # No hinge loss
  hinge_coef:
    value: 0

  seed:
    values: [1, 2]

  # Fixed config
  steps:
    value: 500000
  eval_freq:
    value: 20000
  compile:
    value: true
  dual_policy_enabled:
    value: true
  eval_mean_head_reduce:
    value: true

  # Other old defaults
  value_update_freq:
    value: 4
  planner_num_dynamics_heads:
    value: 4
  policy_head_reduce:
    value: min
  planner_head_reduce:
    value: max
  planner_use_all_heads_eval:
    value: true

  enable_wandb:
    value: true
  wandb_project:
    value: tdmpc2-tdmpc2
  wandb_entity:
    value: thomasevers9
  save_video:
    value: false
