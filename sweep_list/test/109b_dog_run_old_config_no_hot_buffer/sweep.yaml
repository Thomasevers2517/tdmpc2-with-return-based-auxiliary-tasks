program: tdmpc2/train.py
project: tdmpc2-tdmpc2
method: grid
command:
  - ${env}
  - python
  - ${program}
  - ${args_no_hyphens}
parameters:
  #############################################
  # Sweep 109b: Dog-Run Old Config (No Hot Buffer)
  # 
  # Same as 109 but with hot_buffer_enabled: false
  # to isolate whether hot buffer is the difference.
  #
  # Total: 1 config Ã— 2 seeds = 2 runs
  #############################################

  task:
    value: dog-run

  # Jacobian correction = 1.0 (same as old use_scaled_entropy: false)
  jacobian_correction_scale:
    value: 1.0

  # Entropy: 3e-5 is equivalent to old 3e-3 / action_dim (38 for dog)
  start_entropy_coeff:
    value: 3e-5

  end_entropy_coeff:
    value: 1e-6

  # No entropy decay (like old sweep)
  start_dynamic_entropy_ratio:
    value: 1.0

  # TD bootstrap: min (old behavior, td_bootstrap_mode didn't exist)
  td_bootstrap_mode:
    value: min

  # Value disagreement penalty (tested in old sweep, use moderate value)
  policy_lambda_value_disagreement:
    value: 1.0

  optimistic_policy_lambda_value_disagreement:
    value: 0.0

  # Old config values
  num_q:
    value: 2

  num_reward_heads:
    value: 1

  # HOT BUFFER DISABLED - key difference from 109
  hot_buffer_enabled:
    value: false

  temperature:
    value: 0.1

  # Dynamics prior was disabled back then
  dynamics_prior_enabled:
    value: false

  # No hinge loss
  hinge_coef:
    value: 0

  seed:
    values: [1, 2]

  # Fixed config
  steps:
    value: 500000
  eval_freq:
    value: 20000
  compile:
    value: true
  dual_policy_enabled:
    value: true
  eval_mean_head_reduce:
    value: true

  # Other old defaults
  value_update_freq:
    value: 4
  planner_num_dynamics_heads:
    value: 4
  policy_head_reduce:
    value: min
  planner_head_reduce:
    value: max
  planner_use_all_heads_eval:
    value: true

  enable_wandb:
    value: true
  wandb_project:
    value: tdmpc2-tdmpc2
  wandb_entity:
    value: thomasevers9
  save_video:
    value: false
