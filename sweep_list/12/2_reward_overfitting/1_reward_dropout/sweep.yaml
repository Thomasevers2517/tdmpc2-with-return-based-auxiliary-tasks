# Sweep 12.2a: Reward overfitting — dropout on reward heads
# 2 tasks × 3 seeds = 6 runs
#
# Baseline comparison: reward_dropout_enabled=false is in sweep 12.1 runs
# with value_prior_scale=0.1, td_target_std_coef=1.
# Here we enable reward dropout to see if it reduces reward head overfitting.

program: tdmpc2/train.py
project: tdmpc2-tdmpc2
method: grid
command:
  - ${env}
  - python
  - ${program}
  - ${args_no_hyphens}
parameters:
  task:
    values:
      - hopper-hop
      - dog-run

  seed:
    values: [1, 2, 3]

  # --- Key change: enable reward dropout ---
  reward_dropout_enabled:
    value: true

  # --- Fixed (optimistic TD + modest prior, matching 12.1 baseline) ---
  td_target_std_coef:
    value: 1
  value_prior_scale:
    value: 0.1
  num_reward_heads:
    value: 2

  num_q:
    value: 5
  steps:
    value: 100000
  rollout_head_strategy:
    value: single
  num_rollouts:
    value: 1

  # Logging
  enable_wandb:
    value: true
  wandb_project:
    value: tdmpc2-tdmpc2
  wandb_entity:
    value: thomasevers9
  compile:
    value: true
  save_video:
    value: false
