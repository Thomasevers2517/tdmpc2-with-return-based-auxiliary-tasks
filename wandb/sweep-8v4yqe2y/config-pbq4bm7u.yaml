wandb_version: 1

compile:
  value: true
dropout:
  value: 0
ema_value_planning:
  value: true
enable_wandb:
  value: true
enc_lr_step_ratio:
  value: 0.5
end_dynamic_entropy_ratio:
  value: 1
entropy_action_dim_power:
  value: 1
eval_freq:
  value: 10000
eval_mean_head_reduce:
  value: false
fix_kl_order:
  value: true
hinge_coef:
  value: 0
iterations:
  value: 6
jacobian_correction_scale:
  value: 1
local_td_bootstrap:
  value: true
multi_gamma_loss_weight:
  value: 0
num_enc_layers:
  value: 2
num_pi_trajs:
  value: 24
num_q:
  value: 2
num_reward_heads:
  value: 2
num_samples:
  value: 512
num_value_layers:
  value: 2
optimistic_entropy_mult:
  value: 100
optimistic_policy_optimization_method:
  value: distillation
optimistic_policy_value_std_coef:
  value: 1
pi_update_freq:
  value: 1
planner_num_dynamics_heads:
  value: 2
planner_use_all_heads_eval:
  value: true
planner_value_std_coef_eval:
  value: -1
planner_value_std_coef_train:
  value: 0.3
policy_optimization_method:
  value: distillation
policy_value_std_coef:
  value: 0
prior_hidden_div:
  value: 16
prior_logit_scale:
  value: 1
prior_scale:
  value: 0.1
reanalyze_batch_size:
  value: 32
reanalyze_horizon:
  value: 3
reanalyze_interval:
  value: 5
reanalyze_iterations:
  value: 4
reanalyze_num_elites:
  value: 64
reanalyze_num_pi_trajs:
  value: 24
reanalyze_num_samples:
  value: 512
reanalyze_temperature:
  value: 0.5
reanalyze_value_std_coef:
  value: 0
save_video:
  value: false
seed:
  value: 1
start_dynamic_entropy_ratio:
  value: 1
start_entropy_coeff:
  value: 0
steps:
  value: 100000
task:
  value: quadruped-walk
td_target_dynamics_reduction:
  value: mean
td_target_std_coef:
  value: 0
utd_ratio:
  value: 4
value_update_freq:
  value: 1
wandb_entity:
  value: thomasevers9
wandb_project:
  value: tdmpc2-tdmpc2
